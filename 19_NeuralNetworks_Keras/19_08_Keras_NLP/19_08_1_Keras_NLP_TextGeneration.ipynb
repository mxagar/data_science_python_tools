{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad9043-625d-4c03-bf8c-e4c024c50f28",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94865b4-2a5b-444b-89bd-a9e418bb44e7",
   "metadata": {},
   "source": [
    "This section is based on the following blog post by Andrej Karpathy:\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "The code used by Karpathy for the article is on Github:\n",
    "\n",
    "[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "Basically, it is a **character-level language model**; astonishingly, the network will learn to create text, even being trained on a character level! \n",
    "\n",
    "The basic idea is shown on the following picture from Karpathy's blog post:\n",
    "\n",
    "![Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`](pics/charseq_karpathy.jpeg)\n",
    "*Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`.*\n",
    "\n",
    "The current notebook is about creating a simplified project, similar to the one described in the article, with the following goal: Given a sequence of characters, predict the same sequence shifted one character: e.g., `[h,e,l,l] (input) -> [e,l,l,o] (prediction)`.\n",
    "\n",
    "Some points to consider:\n",
    "- We are going to use the complete works by Shakespeare for training. The reansons are: (1) we have more than one million characters in the text and (2) the text is very well structured. However, any long text could be used, look at [gutenberg.org](https://www.gutenberg.org)\n",
    "- We are going to create a one-hot encoding for the alphabet characters and punctuation; then, we are going to use an embedding to compress those one-hot vectors.\n",
    "\n",
    "Steps followed:\n",
    "1. Load text/data; a large dataset with millions of characters is required\n",
    "2. Text processing and vectorization: integers assigned to letterns and symbols (e.g., punctuation)\n",
    "3. Create batches: create long enough sequences to learn relationships, but not too long to avoid noise\n",
    "4. Crate the model: we'll have 3 layers\n",
    "    - Embedding layer: one-hot encoding vectors are compressed to a smaller space of fixed size (dimensions)\n",
    "    - GRU layer: a simplified version of LSTM units (i.e., with fewer parameters), which leads to better results (see RNN folder: `../19_07_Keras_RNN`)\n",
    "    - Dense layer: probabilities per character\n",
    "5. Train the model\n",
    "6. Inference\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A nice description of what embeddings are is given in this video on the DotCSV Youtube channel:\n",
    "\n",
    "[INTRO al Natural Language Processing (NLP) #2 - ¿Qué es un EMBEDDING?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)\n",
    "\n",
    "Embeddings are not exclusive to language, but are commonly used in it, thanks to approaches like `word2Vec`, published in\n",
    "\n",
    "\"Efficient Estimation of Word Representations in Vector Space\", Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013, Google.\n",
    "\n",
    "The idea is that, first, **we create a one-hot encoding to represent our vocabulary** in order to start working on the text (**note that one-hot encoding can be also represented as categorical integers**); the size of the one-hot vector is the size of the vocabulary (i.e., the number of words, say 10k). This representation has several problems, such as:\n",
    "- It is large and sparse.\n",
    "- Words that are close to each other semantically ar ethe same dinstance apart as words that should be far away.\n",
    "\n",
    "In order to solve those issues, a shallow neural net can be applied to the one-hot vectors to compressed them to a space with less dimensions (e.g., 300) but continuous values. For example, here we map a 7-dim vocabulary space to a 2D embedding space.\n",
    "\n",
    "`[0,0,0,1,0,0,0] -> 4 (/7) -> [0.54, 0.01]`\n",
    "\n",
    "The nice thing is that vectors in the embedding space that are close to each other are in the reality semantically close to each other. Thus, we could start applying typical algebra operations on them, in such a way that `V(king) - V(man) + V(woman)` should be close to `V(queen)`. We can also apply dimensionality reduction techniques (e.g., PCA) and visualize the words in the embedding (e.g., in 3D space).\n",
    "\n",
    "One of the issues of embedding spaces in NLP is polysemia: when a words has different meanings and the context matters, the same vector should be split into different vectors. Research is being done to address that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c34f9-7bc3-48e4-be44-13d438266a6e",
   "metadata": {},
   "source": [
    "## 1. Load Text/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9922bba3-b9e7-4051-b38c-90505a93f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f679a73a-b7c3-4dc7-bf39-4006df898347",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt','r') as f:\n",
    "    #lines = f.readlines() \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67888193-4e6e-453d-8bae-c8d18d7372b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The text has symbols in it, such as \\n\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d942771f-c20c-4fb6-80f5-9dfcd5ce5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houldst not abhor my state.\n",
      "    If thy unworthiness raised love in me,\n",
      "    More worthy I to be beloved of thee.\n",
      "\n",
      "\n",
      "                     151\n",
      "  Love is too young to know what conscience is,  \n",
      "  Yet who knows not conscience is born of love?\n",
      "  Then gentle cheater urge not my amiss,\n",
      "  Lest guilty of my faults thy sweet self prove.\n",
      "  For thou betraying me, I do betray\n",
      "  My nobler part to my gross body's \n"
     ]
    }
   ],
   "source": [
    "# If we print it, the symbols are interpreted\n",
    "print(text[100100:100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbafe5-66c5-4c3b-a29f-c22006e1a65d",
   "metadata": {},
   "source": [
    "## 2. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "945d39bc-0d73-410a-8d29-ee6d0e34f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a set with all characters and symbols\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a580d1f-0ad8-4093-b372-58509b2b99eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8ff7492-7d1b-4e22-a0e8-906c37582580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of characters/symbols we have - important for the final dense layer\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cc412af-60f6-4cee-902c-9152bee0f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "# Now, we need to bidirectionally associate each character in the vocabulary\n",
    "# with a number (related to one-hot encoding):\n",
    "# character <-> number\n",
    "# We can do that with enumerate and dictionaries\n",
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af337315-5c98-4270-b363-c7d0ac6992ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following that, we create a dictionary with comprehension\n",
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8059957c-579c-4ffc-80ee-064b9a8fed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional association\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b4f63b5-ab18-4c79-bc7f-8c1a3caf1fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e0e2caf-960d-47ab-80ca-fcfab0bf2433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cc0a6e8-0e9e-490b-9878-fbd5700c07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, with those two vectors, we can encodde our text!\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc10d3b2-bc2c-4708-a7d1-9cd62b539ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check that we have several millions of characters (necessary for good enough results)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "529a7230-cf54-45bd-bcbc-dbb862b9af83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"d buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can compare the regular text and the encoded one\n",
    "text[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e230d19c-dc9d-4b3f-b7f6-f4c2fae0763f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59,  1, 57, 76, 73, 64, 60, 74, 75,  1, 75, 63, 80,  1, 58, 70, 69,\n",
       "       75, 60, 69, 75,  8,  0,  1,  1, 26, 69, 59,  1, 75, 60, 69, 59, 60,\n",
       "       73,  1, 58, 63, 76, 73, 67,  1, 68, 56, 66,  5, 74, 75,  1, 78, 56,\n",
       "       74, 75, 60,  1, 64, 69,  1, 69, 64, 62, 62, 56, 73, 59, 64, 69, 62,\n",
       "       21,  0,  1,  1,  1,  1, 41, 64, 75, 80,  1, 75, 63, 60,  1, 78, 70,\n",
       "       73, 67, 59,  8,  1, 70, 73,  1, 60, 67, 74, 60,  1, 75, 63, 64, 74,\n",
       "        1, 62, 67, 76, 75, 75, 70, 69,  1, 57, 60,  8,  0,  1,  1,  1,  1,\n",
       "       45, 70,  1, 60, 56, 75,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  5,\n",
       "       74,  1, 59, 76, 60,  8,  1, 57, 80,  1, 75, 63, 60,  1, 62, 73, 56,\n",
       "       77, 60,  1, 56, 69, 59,  1, 75, 63, 60, 60, 10,  0,  0,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1, 13,  0,  1,  1, 48, 63, 60, 69,  1, 61, 70, 73, 75, 80,  1,\n",
       "       78, 64, 69, 75, 60, 73, 74,  1, 74, 63, 56, 67, 67,  1, 57, 60, 74,\n",
       "       64, 60, 62, 60,  1, 75, 63, 80,  1, 57, 73, 70, 78,  8,  0,  1,  1,\n",
       "       26, 69, 59,  1, 59, 64, 62,  1, 59, 60, 60, 71,  1, 75, 73, 60, 69,\n",
       "       58, 63, 60, 74,  1, 64, 69,  1, 75, 63, 80,  1, 57, 60, 56, 76, 75,\n",
       "       80,  5, 74,  1, 61, 64, 60, 67, 59,  8,  0,  1,  1, 45, 63, 80,  1,\n",
       "       80, 70, 76, 75, 63,  5, 74,  1, 71, 73, 70, 76, 59,  1, 67, 64, 77,\n",
       "       60, 73, 80,  1, 74, 70,  1, 62, 56, 81, 60, 59,  1, 70, 69,  1, 69,\n",
       "       70, 78,  8,  0,  1,  1, 48, 64, 67, 67,  1, 57, 60,  1, 56,  1, 75,\n",
       "       56, 75, 75, 60, 73, 60, 59,  1, 78, 60, 60, 59,  1, 70, 61,  1, 74,\n",
       "       68, 56, 67, 67,  1, 78, 70, 73, 75, 63,  1, 63, 60, 67, 59, 21,  1,\n",
       "        1,  0,  1,  1, 45, 63, 60, 69,  1, 57, 60, 64, 69, 62,  1, 56, 74,\n",
       "       66, 60, 59,  8,  1, 78, 63, 60, 73, 60,  1, 56, 67, 67,  1, 75, 63,\n",
       "       80,  1, 57, 60, 56, 76, 75, 80,  1, 67, 64, 60, 74,  8,  0,  1,  1,\n",
       "       48, 63, 60, 73, 60,  1, 56, 67, 67,  1, 75, 63, 60,  1, 75, 73, 60,\n",
       "       56, 74, 76, 73, 60,  1, 70, 61,  1, 75, 63, 80,  1, 67, 76, 74, 75,\n",
       "       80,  1, 59, 56, 80, 74, 22,  0,  1,  1, 45, 70,  1, 74, 56, 80,  1,\n",
       "       78, 64, 75, 63, 64, 69,  1, 75, 63, 64, 69, 60,  1, 70, 78, 69,  1,\n",
       "       59, 60, 60, 71,  1, 74, 76])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[500:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a51fa-4be0-4127-8d46-034ca76b91f8",
   "metadata": {},
   "source": [
    "## 3. Creating Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff4626-1e56-4fd4-9271-6d00a5882a61",
   "metadata": {},
   "source": [
    "Some important notes:\n",
    "- An input sequence needs to have enough characters to contain the general structures of the text: Shakespeare has lines of around 40 characters and a rhyme every second line; thus, we take a sequence length of `3x40 = 120`.\n",
    "- The idea is that we feed in a sequence and the output is the same sequence except the first character and additionally the next character which is most probable given the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a656a148-1cf5-4e7c-927b-58f72684f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "# We analyze which sequence length to take\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77877d92-75c0-4c26-b581-54df8b90202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"From fairest creatures we desire increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a779779-096e-412c-9553-765e80fba8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c398b-12e9-4c73-9690-bdd72e5eaf0d",
   "metadata": {},
   "source": [
    "### Create Training Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aefdfed-4f78-479b-b4c1-541ec34e869f",
   "metadata": {},
   "source": [
    "The complete text needs to be partitioned in sequences and for each input sequence we need to associate a ground truth output sequence for training:\n",
    "\n",
    "`in: 'Hello, my name i' -> out: 'ello, my name is'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9e384b9-c79d-4539-907a-53ea9341fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 120 # motivation explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "29587f68-8e5d-4794-a429-962b76726d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_seq = len(text)//(seq_len+1) # we ignore the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7d78a6e9-54a3-48f0-b4e3-9b7093f67b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0142d664-dcdd-4224-8f12-5b721af8d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a TF dataset which we can slice as written above\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e5d5362b-a1b1-4f8e-8cc5-145e7c028208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "w\n"
     ]
    }
   ],
   "source": [
    "# That TF dataset has several call functions\n",
    "# .take() takes the number of characters we specify\n",
    "# .batch() creates sequences of the size/length we specify\n",
    "# For example:\n",
    "for i in char_dataset.take(50):\n",
    "    print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff7edc53-def3-4072-bc8b-a0f0b02f248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That TF dataset has several call functions\n",
    "# .batch() creates the number of sequences we'd like\n",
    "# we use seq_len+1 because we want to have both in and out texts in a sequence, shifted by a character\n",
    "# drop_remainder=True means the final chars that do not fit in 120 are dropped\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "677b57ad-5c3d-4a29-bf6a-5082f82d3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all sequences, we need to generate for each sequence\n",
    "# the input and output/target text pairs to use during training\n",
    "# For that, the following function is mapped to the sequences to obtain our dataset\n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1] # Hell\n",
    "    target_txt = seq[1:] # ello\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "84a3c6a1-885e-441a-9238-f248af6ed9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset is a collection of pairs\n",
    "# Each pair has the input and output sequences shifted by 1 character\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "467dd032-801b-47ea-8d29-f8dfb2f99cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "# We can takea pair and print it\n",
    "for input_txt,target_txt in dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4a0da-4d5b-4447-a56f-f4d2e374c546",
   "metadata": {},
   "source": [
    "### Shuffling the Batches of Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bd0b9-5a65-470e-be05-becc6c299538",
   "metadata": {},
   "source": [
    "After creating the in/out sequences, we need to create batches of those sequence pairs.\n",
    "Addtionally, it is a good practice to shuffle those batches to avoid aóverfitting a section in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9256b451-b923-415a-8db7-1328fb3d05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "# Buffer size: see documentation - shuffling happens in groups of buffer size (better behavior)\n",
    "buffer_size = 10000\n",
    "# Remainder is dropped\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4da85557-4a06-4c65-bdf4-48e3d66b0685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 128, 128, 128, 120), (128, 128, 128, 128, 120)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf13924-8df4-4936-8418-19105206f408",
   "metadata": {},
   "source": [
    "## 4. Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5beab-50f4-4452-a0db-b16f53ff9853",
   "metadata": {},
   "source": [
    "I understand that the model defined in the course is not the one proposed by Karpathy; instead, Portilla seems to take a similar architecture as in [DeepMoji](https://deepmoji.mit.edu), which is available on [Github](https://github.com/bfelbo/DeepMoji).\n",
    "\n",
    "In our architecture, we have the following layers:\n",
    "- An **embedding** layer which compresses our vocabulary (84D) to a smaller embedding space (64D); embeddings improve RNN performance for NLP\n",
    "- RNN: LSTM units / **GRU** units: 1026\n",
    "- A **Dense** layer of the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15342b82-cdd9-487c-9038-54e30050ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3647045-172a-4bfc-90d0-948ee594c824",
   "metadata": {},
   "source": [
    "### Loss Function: Sparse Categorical Cross-Entropy\n",
    "\n",
    "As stated in the following post\n",
    "\n",
    "[Stackexchange](https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy)\n",
    "\n",
    "we sould \"use **sparse categorical crossentropy** when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and **categorical crossentropy** when one sample can have multiple classes or labels are soft probabilities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aecda8eb-5a8f-4dfc-96f3-0cc83d9f174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5cd7578d-0a52-40d7-ab38-96bb2b73b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_categorical_crossentropy in module tensorflow.python.keras.losses:\n",
      "\n",
      "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1c53f864-e583-48c6-bfc7-5b70c397edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the default sparse_categorical_crossentropy does not work with on-hot encoded parameters\n",
    "# we need to create our own loss function\n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    # Since we have one-hot encoding: from_logits=True\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15168d78-4c0d-4503-9cab-18166e2aec31",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "11a27a34-ba86-426a-98be-4f74762821be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "87051086-2591-4c23-b906-fca6cdb7d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the model with a custom function\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    # Shift+TAB for documentation\n",
    "    # Glorot stands for the Xavier initialization, after Xavier Glorot\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    # We use our custom loss function\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c2e3dd4d-5c37-46a6-ab27-df1e00f6a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocab_size = vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1fc5eff7-32f7-405f-b3db-a51029b2a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5d4c0-2b0d-4d9f-99a6-8a6a1bb59f2a",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda885bd-eef0-41d1-aaa3-e37a131db103",
   "metadata": {},
   "source": [
    "**Important note:** we see that our RNN model has around 3.5M parameters, which is not viable to train on a laptop. Instead, a computer with powerful GPUs is required. Therefore, Google Colab is used in the course.\n",
    "\n",
    "Follow these instructions:\n",
    "- Open [Google Colab](https://colab.research.google.com/) and log in\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786edb9-4dda-43ed-966f-5722187e97e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
