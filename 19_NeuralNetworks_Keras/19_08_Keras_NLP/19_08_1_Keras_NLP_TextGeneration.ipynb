{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad9043-625d-4c03-bf8c-e4c024c50f28",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94865b4-2a5b-444b-89bd-a9e418bb44e7",
   "metadata": {},
   "source": [
    "This section is based on the following blog post by Andrej Karpathy:\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "The code used by Karpathy for the article is on Github:\n",
    "\n",
    "[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "Basically, it is a **character-level language model**; astonishingly, the network will learn to create text, even being trained on a character level!\n",
    "\n",
    "The basic idea is show on th efollowing picture from Karpathy's blog post:\n",
    "\n",
    "![Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`](pics/charseq_karpathy.jpeg)\n",
    "*Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`.*\n",
    "\n",
    "The current notebook is about creating a simplified project, skmilar to the one described in the article, with the following goal: Given a sequence of characters, predict the same sequence shifted one character: e.g., `[h,e,l,l] -> [e,l,l,o]`.\n",
    "\n",
    "Some points to consider:\n",
    "- We are going to use the complete works by Shakespeare for training.\n",
    "- We are going to create a one-hot encoding for the alphabet characters and punctuation.\n",
    "\n",
    "Steps followed:\n",
    "1. Load text/data; a large dataset with millions of characters is required\n",
    "2. Text processing and vectorization: integers assigned to letterns and symbols (e.g., punctuation)\n",
    "3. Create batches: create long enough sequences to learn relationships, but not too long to avoid noise\n",
    "4. Crate the model: we'll have 3 layers\n",
    "    - Embedding layer: one-hot encoding vectors are compressed to a smaller space of fixed size (dimensions)\n",
    "    - GRU layer: a simplified version of LSTM units (i.e., with fewer parameters), which leads to better results (see RNN folder: `../19_07_Keras_RNN`)\n",
    "    - Dense layer: probabilities per character\n",
    "5. Train the model\n",
    "6. Inference\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A nice description of what embeddings are is given in this video on the DotCSV Youtube channel:\n",
    "\n",
    "[INTRO al Natural Language Processing (NLP) #2 - ¿Qué es un EMBEDDING?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)\n",
    "\n",
    "Embeddings are not exclusive to language, but are commonly used in it, thanks to approaches like `word2Vec`, published in\n",
    "\n",
    "\"Efficient Estimation of Word Representations in Vector Space\", Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013, Google.\n",
    "\n",
    "The idea is that, first, **we create a one-hot encoding to represent our vocabulary** in order to start working on the text; the size of the one-hot vector is the size of the vocabulary (i.e., the number of words, say 10k). This representation has several problems, such as:\n",
    "- It is large and sparse.\n",
    "- Words that are close to each other semantically ar ethe same dinstance apart as words that should be far away.\n",
    "\n",
    "In order to solve those issues, a shallow neural net can be applied to the one-hot vectors to compressed them to a space with less dimensions (e.g., 300) but continuous values:\n",
    "\n",
    "`[0,0,0,1,0,0,0] -> [0.54, 0.01]`\n",
    "\n",
    "The nice thing is that vectors in the embedding space that are close to each other are in the reality semantically close to each other. Thus, we could start applying typical algebra operations on them, in such a way that `V(king) - V(man) + V(woman)` should be close to `V(queen)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c34f9-7bc3-48e4-be44-13d438266a6e",
   "metadata": {},
   "source": [
    "## 1. Load Text/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9922bba3-b9e7-4051-b38c-90505a93f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679a73a-b7c3-4dc7-bf39-4006df898347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
