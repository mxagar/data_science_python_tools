{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad9043-625d-4c03-bf8c-e4c024c50f28",
   "metadata": {
    "id": "42ad9043-625d-4c03-bf8c-e4c024c50f28"
   },
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94865b4-2a5b-444b-89bd-a9e418bb44e7",
   "metadata": {
    "id": "d94865b4-2a5b-444b-89bd-a9e418bb44e7"
   },
   "source": [
    "This section is based on the following blog post by Andrej Karpathy:\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "The code used by Karpathy for the article is on Github:\n",
    "\n",
    "[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "Basically, it is a **character-level language model**; astonishingly, the network will learn to create text, even being trained on a character level! \n",
    "\n",
    "The basic idea is shown on the following picture from Karpathy's blog post:\n",
    "\n",
    "![Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`](pics/charseq_karpathy.jpeg)\n",
    "*Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`.*\n",
    "\n",
    "The current notebook is about creating a simplified project, similar to the one described in the article, with the following goal: Given a sequence of characters, predict the same sequence shifted one character: e.g., `[h,e,l,l] (input) -> [e,l,l,o] (prediction)`.\n",
    "\n",
    "Some points to consider:\n",
    "- We are going to use the complete works by Shakespeare for training. The reansons are: (1) we have more than one million characters in the text and (2) the text is very well structured. However, any long text could be used, look at [gutenberg.org](https://www.gutenberg.org)\n",
    "- We are going to create a one-hot encoding for the alphabet characters and punctuation; then, we are going to use an embedding to compress those one-hot vectors.\n",
    "\n",
    "Steps followed:\n",
    "1. Load text/data; a large dataset with millions of characters is required\n",
    "2. Text processing and vectorization: integers assigned to letterns and symbols (e.g., punctuation)\n",
    "3. Create batches: create long enough sequences to learn relationships, but not too long to avoid noise\n",
    "4. Crate the model: we'll have 3 layers\n",
    "    - Embedding layer: one-hot encoding vectors are compressed to a smaller space of fixed size (dimensions)\n",
    "    - GRU layer: a simplified version of LSTM units (i.e., with fewer parameters), which leads to better results (see RNN folder: `../19_07_Keras_RNN`)\n",
    "    - Dense layer: probabilities per character\n",
    "5. Train the model\n",
    "6. Inference\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A nice description of what embeddings are is given in this video on the DotCSV Youtube channel:\n",
    "\n",
    "[INTRO al Natural Language Processing (NLP) #2 - ¿Qué es un EMBEDDING?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)\n",
    "\n",
    "Embeddings are not exclusive to language, but are commonly used in it, thanks to approaches like `word2Vec`, published in\n",
    "\n",
    "\"Efficient Estimation of Word Representations in Vector Space\", Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013, Google.\n",
    "\n",
    "The idea is that, first, **we create a one-hot encoding to represent our vocabulary** in order to start working on the text (**note that one-hot encoding can be also represented as categorical integers**); the size of the one-hot vector is the size of the vocabulary (i.e., the number of words, say 10k). This representation has several problems, such as:\n",
    "- It is large and sparse.\n",
    "- Words that are close to each other semantically ar ethe same dinstance apart as words that should be far away.\n",
    "\n",
    "In order to solve those issues, a shallow neural net can be applied to the one-hot vectors to compressed them to a space with less dimensions (e.g., 300) but continuous values. For example, here we map a 7-dim vocabulary space to a 2D embedding space.\n",
    "\n",
    "`[0,0,0,1,0,0,0] -> 4 (/7) -> [0.54, 0.01]`\n",
    "\n",
    "The nice thing is that vectors in the embedding space that are close to each other are in the reality semantically close to each other. Thus, we could start applying typical algebra operations on them, in such a way that `V(king) - V(man) + V(woman)` should be close to `V(queen)`. We can also apply dimensionality reduction techniques (e.g., PCA) and visualize the words in the embedding (e.g., in 3D space).\n",
    "\n",
    "One of the issues of embedding spaces in NLP is polysemia: when a words has different meanings and the context matters, the same vector should be split into different vectors. Research is being done to address that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c34f9-7bc3-48e4-be44-13d438266a6e",
   "metadata": {
    "id": "175c34f9-7bc3-48e4-be44-13d438266a6e"
   },
   "source": [
    "## 1. Load Text/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9922bba3-b9e7-4051-b38c-90505a93f367",
   "metadata": {
    "id": "9922bba3-b9e7-4051-b38c-90505a93f367"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ZsauRGuYVIY-",
   "metadata": {
    "id": "ZsauRGuYVIY-"
   },
   "outputs": [],
   "source": [
    "# This line is for Google Colab, ignore it if notebook tf version is >= 2\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aFMTrenyVJR0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "aFMTrenyVJR0",
    "outputId": "14fd4b46-ff07-4a66-bba9-14c5c995df08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line is to check that the TF version is >= 2\n",
    "# Important for Gooogle Colab\n",
    "# If not, we can uncomment the magic command before importing tensorflow above\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f679a73a-b7c3-4dc7-bf39-4006df898347",
   "metadata": {
    "id": "f679a73a-b7c3-4dc7-bf39-4006df898347"
   },
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt','r') as f:\n",
    "    #lines = f.readlines() \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67888193-4e6e-453d-8bae-c8d18d7372b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "67888193-4e6e-453d-8bae-c8d18d7372b0",
    "outputId": "665bc8cf-7155-41d3-a360-a282414bc03a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The text has symbols in it, such as \\n\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d942771f-c20c-4fb6-80f5-9dfcd5ce5549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d942771f-c20c-4fb6-80f5-9dfcd5ce5549",
    "outputId": "a0f58e0f-802a-45aa-b5ac-f62dddca2e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houldst not abhor my state.\n",
      "    If thy unworthiness raised love in me,\n",
      "    More worthy I to be beloved of thee.\n",
      "\n",
      "\n",
      "                     151\n",
      "  Love is too young to know what conscience is,  \n",
      "  Yet who knows not conscience is born of love?\n",
      "  Then gentle cheater urge not my amiss,\n",
      "  Lest guilty of my faults thy sweet self prove.\n",
      "  For thou betraying me, I do betray\n",
      "  My nobler part to my gross body's \n"
     ]
    }
   ],
   "source": [
    "# If we print it, the symbols are interpreted\n",
    "print(text[100100:100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbafe5-66c5-4c3b-a29f-c22006e1a65d",
   "metadata": {
    "id": "10fbafe5-66c5-4c3b-a29f-c22006e1a65d"
   },
   "source": [
    "## 2. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "945d39bc-0d73-410a-8d29-ee6d0e34f44a",
   "metadata": {
    "id": "945d39bc-0d73-410a-8d29-ee6d0e34f44a"
   },
   "outputs": [],
   "source": [
    "# We create a set with all characters and symbols\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a580d1f-0ad8-4093-b372-58509b2b99eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a580d1f-0ad8-4093-b372-58509b2b99eb",
    "outputId": "8a46cfad-75b0-441a-9397-c569517144c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8ff7492-7d1b-4e22-a0e8-906c37582580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8ff7492-7d1b-4e22-a0e8-906c37582580",
    "outputId": "e5fb4054-a4f8-4419-a95b-e4ad7a3bd7ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of characters/symbols we have - important for the final dense layer\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cc412af-60f6-4cee-902c-9152bee0f7d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cc412af-60f6-4cee-902c-9152bee0f7d8",
    "outputId": "3fa2b898-215e-4e3d-87e6-87af1f845ec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "# Now, we need to bidirectionally associate each character in the vocabulary\n",
    "# with a number (related to one-hot encoding):\n",
    "# character <-> number\n",
    "# We can do that with enumerate and dictionaries\n",
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af337315-5c98-4270-b363-c7d0ac6992ca",
   "metadata": {
    "id": "af337315-5c98-4270-b363-c7d0ac6992ca"
   },
   "outputs": [],
   "source": [
    "# Following that, we create a dictionary with comprehension\n",
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8059957c-579c-4ffc-80ee-064b9a8fed77",
   "metadata": {
    "id": "8059957c-579c-4ffc-80ee-064b9a8fed77"
   },
   "outputs": [],
   "source": [
    "# Bidirectional association\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b4f63b5-ab18-4c79-bc7f-8c1a3caf1fdb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b4f63b5-ab18-4c79-bc7f-8c1a3caf1fdb",
    "outputId": "47978adc-4be5-43da-ec18-bcf87469deb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e0e2caf-960d-47ab-80ca-fcfab0bf2433",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "9e0e2caf-960d-47ab-80ca-fcfab0bf2433",
    "outputId": "c6df5bd7-4579-40b8-db6a-651dfc407c5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3cc0a6e8-0e9e-490b-9878-fbd5700c07e8",
   "metadata": {
    "id": "3cc0a6e8-0e9e-490b-9878-fbd5700c07e8"
   },
   "outputs": [],
   "source": [
    "# Now, with those two vectors, we can encodde our text!\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc10d3b2-bc2c-4708-a7d1-9cd62b539ab1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc10d3b2-bc2c-4708-a7d1-9cd62b539ab1",
    "outputId": "d7931af8-5d1d-46f5-f6cf-b7d6c47a354c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check that we have several millions of characters (necessary for good enough results)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "529a7230-cf54-45bd-bcbc-dbb862b9af83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "529a7230-cf54-45bd-bcbc-dbb862b9af83",
    "outputId": "ba3a9346-10e2-488e-ef82-46e3549ce7f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"d buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can compare the regular text and the encoded one\n",
    "text[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e230d19c-dc9d-4b3f-b7f6-f4c2fae0763f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e230d19c-dc9d-4b3f-b7f6-f4c2fae0763f",
    "outputId": "eb1134a8-7341-4d66-9c8a-558a36d5003c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59,  1, 57, 76, 73, 64, 60, 74, 75,  1, 75, 63, 80,  1, 58, 70, 69,\n",
       "       75, 60, 69, 75,  8,  0,  1,  1, 26, 69, 59,  1, 75, 60, 69, 59, 60,\n",
       "       73,  1, 58, 63, 76, 73, 67,  1, 68, 56, 66,  5, 74, 75,  1, 78, 56,\n",
       "       74, 75, 60,  1, 64, 69,  1, 69, 64, 62, 62, 56, 73, 59, 64, 69, 62,\n",
       "       21,  0,  1,  1,  1,  1, 41, 64, 75, 80,  1, 75, 63, 60,  1, 78, 70,\n",
       "       73, 67, 59,  8,  1, 70, 73,  1, 60, 67, 74, 60,  1, 75, 63, 64, 74,\n",
       "        1, 62, 67, 76, 75, 75, 70, 69,  1, 57, 60,  8,  0,  1,  1,  1,  1,\n",
       "       45, 70,  1, 60, 56, 75,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  5,\n",
       "       74,  1, 59, 76, 60,  8,  1, 57, 80,  1, 75, 63, 60,  1, 62, 73, 56,\n",
       "       77, 60,  1, 56, 69, 59,  1, 75, 63, 60, 60, 10,  0,  0,  0,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1, 13,  0,  1,  1, 48, 63, 60, 69,  1, 61, 70, 73, 75, 80,  1,\n",
       "       78, 64, 69, 75, 60, 73, 74,  1, 74, 63, 56, 67, 67,  1, 57, 60, 74,\n",
       "       64, 60, 62, 60,  1, 75, 63, 80,  1, 57, 73, 70, 78,  8,  0,  1,  1,\n",
       "       26, 69, 59,  1, 59, 64, 62,  1, 59, 60, 60, 71,  1, 75, 73, 60, 69,\n",
       "       58, 63, 60, 74,  1, 64, 69,  1, 75, 63, 80,  1, 57, 60, 56, 76, 75,\n",
       "       80,  5, 74,  1, 61, 64, 60, 67, 59,  8,  0,  1,  1, 45, 63, 80,  1,\n",
       "       80, 70, 76, 75, 63,  5, 74,  1, 71, 73, 70, 76, 59,  1, 67, 64, 77,\n",
       "       60, 73, 80,  1, 74, 70,  1, 62, 56, 81, 60, 59,  1, 70, 69,  1, 69,\n",
       "       70, 78,  8,  0,  1,  1, 48, 64, 67, 67,  1, 57, 60,  1, 56,  1, 75,\n",
       "       56, 75, 75, 60, 73, 60, 59,  1, 78, 60, 60, 59,  1, 70, 61,  1, 74,\n",
       "       68, 56, 67, 67,  1, 78, 70, 73, 75, 63,  1, 63, 60, 67, 59, 21,  1,\n",
       "        1,  0,  1,  1, 45, 63, 60, 69,  1, 57, 60, 64, 69, 62,  1, 56, 74,\n",
       "       66, 60, 59,  8,  1, 78, 63, 60, 73, 60,  1, 56, 67, 67,  1, 75, 63,\n",
       "       80,  1, 57, 60, 56, 76, 75, 80,  1, 67, 64, 60, 74,  8,  0,  1,  1,\n",
       "       48, 63, 60, 73, 60,  1, 56, 67, 67,  1, 75, 63, 60,  1, 75, 73, 60,\n",
       "       56, 74, 76, 73, 60,  1, 70, 61,  1, 75, 63, 80,  1, 67, 76, 74, 75,\n",
       "       80,  1, 59, 56, 80, 74, 22,  0,  1,  1, 45, 70,  1, 74, 56, 80,  1,\n",
       "       78, 64, 75, 63, 64, 69,  1, 75, 63, 64, 69, 60,  1, 70, 78, 69,  1,\n",
       "       59, 60, 60, 71,  1, 74, 76])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[500:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a51fa-4be0-4127-8d46-034ca76b91f8",
   "metadata": {
    "id": "362a51fa-4be0-4127-8d46-034ca76b91f8"
   },
   "source": [
    "## 3. Creating Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff4626-1e56-4fd4-9271-6d00a5882a61",
   "metadata": {
    "id": "03ff4626-1e56-4fd4-9271-6d00a5882a61"
   },
   "source": [
    "Some important notes:\n",
    "- An input sequence needs to have enough characters to contain the general structures of the text: Shakespeare has lines of around 40 characters and a rhyme every second line; thus, we take a sequence length of `3x40 = 120`.\n",
    "- The idea is that we feed in a sequence and the output is the same sequence except the first character and additionally the next character which is most probable given the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a656a148-1cf5-4e7c-927b-58f72684f009",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a656a148-1cf5-4e7c-927b-58f72684f009",
    "outputId": "33e76857-5132-4390-910b-a506dd9040f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bu\n"
     ]
    }
   ],
   "source": [
    "# We analyze which sequence length to take\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77877d92-75c0-4c26-b581-54df8b90202e",
   "metadata": {
    "id": "77877d92-75c0-4c26-b581-54df8b90202e"
   },
   "outputs": [],
   "source": [
    "line = \"From fairest creatures we desire increase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a779779-096e-412c-9553-765e80fba8a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a779779-096e-412c-9553-765e80fba8a5",
    "outputId": "815059f7-322c-4e97-9e50-11d1e20cb3d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c398b-12e9-4c73-9690-bdd72e5eaf0d",
   "metadata": {
    "id": "278c398b-12e9-4c73-9690-bdd72e5eaf0d"
   },
   "source": [
    "### Create Training Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aefdfed-4f78-479b-b4c1-541ec34e869f",
   "metadata": {
    "id": "8aefdfed-4f78-479b-b4c1-541ec34e869f"
   },
   "source": [
    "The complete text needs to be partitioned in sequences and for each input sequence we need to associate a ground truth output sequence for training:\n",
    "\n",
    "`in: 'Hello, my name i' -> out: 'ello, my name is'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9e384b9-c79d-4539-907a-53ea9341fe68",
   "metadata": {
    "id": "a9e384b9-c79d-4539-907a-53ea9341fe68"
   },
   "outputs": [],
   "source": [
    "seq_len = 120 # motivation explained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "29587f68-8e5d-4794-a429-962b76726d5f",
   "metadata": {
    "id": "29587f68-8e5d-4794-a429-962b76726d5f"
   },
   "outputs": [],
   "source": [
    "total_num_seq = len(text)//(seq_len+1) # we ignore the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d78a6e9-54a3-48f0-b4e3-9b7093f67b7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d78a6e9-54a3-48f0-b4e3-9b7093f67b7c",
    "outputId": "f6f57132-a8f1-41d6-8038-8c1252531be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0142d664-dcdd-4224-8f12-5b721af8d14c",
   "metadata": {
    "id": "0142d664-dcdd-4224-8f12-5b721af8d14c"
   },
   "outputs": [],
   "source": [
    "# We create a TF dataset which we can slice as written above\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5d5362b-a1b1-4f8e-8cc5-145e7c028208",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5d5362b-a1b1-4f8e-8cc5-145e7c028208",
    "outputId": "084c49a2-2854-45b9-e6b1-df2df00c95d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "F\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "w\n"
     ]
    }
   ],
   "source": [
    "# That TF dataset has several call functions\n",
    "# .take() takes the number of characters we specify\n",
    "# .batch() creates sequences of the size/length we specify\n",
    "# For example:\n",
    "for i in char_dataset.take(50):\n",
    "    print(ind_to_char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff7edc53-def3-4072-bc8b-a0f0b02f248b",
   "metadata": {
    "id": "ff7edc53-def3-4072-bc8b-a0f0b02f248b"
   },
   "outputs": [],
   "source": [
    "# That TF dataset has several call functions\n",
    "# .batch() creates the number of sequences we'd like\n",
    "# we use seq_len+1 because we want to have both in and out texts in a sequence, shifted by a character\n",
    "# drop_remainder=True means the final chars that do not fit in 120 are dropped\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "677b57ad-5c3d-4a29-bf6a-5082f82d3ba0",
   "metadata": {
    "id": "677b57ad-5c3d-4a29-bf6a-5082f82d3ba0"
   },
   "outputs": [],
   "source": [
    "# Now that we have all sequences, we need to generate for each sequence\n",
    "# the input and output/target text pairs to use during training\n",
    "# For that, the following function is mapped to the sequences to obtain our dataset\n",
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1] # Hell\n",
    "    target_txt = seq[1:] # ello\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84a3c6a1-885e-441a-9238-f248af6ed9a9",
   "metadata": {
    "id": "84a3c6a1-885e-441a-9238-f248af6ed9a9"
   },
   "outputs": [],
   "source": [
    "# Our dataset is a collection of pairs\n",
    "# Each pair has the input and output sequences shifted by 1 character\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "467dd032-801b-47ea-8d29-f8dfb2f99cdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "467dd032-801b-47ea-8d29-f8dfb2f99cdf",
    "outputId": "5a8eeb77-e7ac-41aa-afaf-20c72f1ff9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
      "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
      "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
      " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
      " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But\n",
      "\n",
      "\n",
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
      "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
      " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
      " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
      "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But \n"
     ]
    }
   ],
   "source": [
    "# We can takea pair and print it\n",
    "for input_txt,target_txt in dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf4a0da-4d5b-4447-a56f-f4d2e374c546",
   "metadata": {
    "id": "bbf4a0da-4d5b-4447-a56f-f4d2e374c546"
   },
   "source": [
    "### Shuffling the Batches of Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025bd0b9-5a65-470e-be05-becc6c299538",
   "metadata": {
    "id": "025bd0b9-5a65-470e-be05-becc6c299538"
   },
   "source": [
    "After creating the in/out sequences, we need to create batches of those sequence pairs.\n",
    "Addtionally, it is a good practice to shuffle those batches to avoid aóverfitting a section in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9256b451-b923-415a-8db7-1328fb3d05cf",
   "metadata": {
    "id": "9256b451-b923-415a-8db7-1328fb3d05cf"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "# Buffer size: see documentation - shuffling happens in groups of buffer size (better behavior)\n",
    "buffer_size = 10000\n",
    "# Remainder is dropped\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4da85557-4a06-4c65-bdf4-48e3d66b0685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4da85557-4a06-4c65-bdf4-48e3d66b0685",
    "outputId": "2e2f1248-b6fd-4e47-ec43-b1b5747daf03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf13924-8df4-4936-8418-19105206f408",
   "metadata": {
    "id": "ebf13924-8df4-4936-8418-19105206f408"
   },
   "source": [
    "## 4. Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a5beab-50f4-4452-a0db-b16f53ff9853",
   "metadata": {
    "id": "b1a5beab-50f4-4452-a0db-b16f53ff9853"
   },
   "source": [
    "I understand that the model defined in the course is not the one proposed by Karpathy; instead, Portilla seems to take a similar architecture as in [DeepMoji](https://deepmoji.mit.edu), which is available on [Github](https://github.com/bfelbo/DeepMoji).\n",
    "\n",
    "In our architecture, we have the following layers:\n",
    "- An **embedding** layer which compresses our vocabulary (84D) to a smaller embedding space (64D); embeddings improve RNN performance for NLP\n",
    "- RNN: LSTM units / **GRU** units: 1026\n",
    "- A **Dense** layer of the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15342b82-cdd9-487c-9038-54e30050ef09",
   "metadata": {
    "id": "15342b82-cdd9-487c-9038-54e30050ef09"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3647045-172a-4bfc-90d0-948ee594c824",
   "metadata": {
    "id": "b3647045-172a-4bfc-90d0-948ee594c824"
   },
   "source": [
    "### Loss Function: Sparse Categorical Cross-Entropy\n",
    "\n",
    "As stated in the following post\n",
    "\n",
    "[Stackexchange](https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy)\n",
    "\n",
    "we sould \"use **sparse categorical crossentropy** when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and **categorical crossentropy** when one sample can have multiple classes or labels are soft probabilities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aecda8eb-5a8f-4dfc-96f3-0cc83d9f174e",
   "metadata": {
    "id": "aecda8eb-5a8f-4dfc-96f3-0cc83d9f174e"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5cd7578d-0a52-40d7-ab38-96bb2b73b50f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cd7578d-0a52-40d7-ab38-96bb2b73b50f",
    "outputId": "480b1949-6832-4ad0-b008-0a72cf1bc0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sparse_categorical_crossentropy in module tensorflow.python.keras.losses:\n",
      "\n",
      "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sparse_categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c53f864-e583-48c6-bfc7-5b70c397edf9",
   "metadata": {
    "id": "1c53f864-e583-48c6-bfc7-5b70c397edf9"
   },
   "outputs": [],
   "source": [
    "# Since the default sparse_categorical_crossentropy does not work with on-hot encoded parameters\n",
    "# we need to create our own loss function\n",
    "def sparse_cat_loss(y_true,y_pred):\n",
    "    # Since we have one-hot encoding: from_logits=True\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15168d78-4c0d-4503-9cab-18166e2aec31",
   "metadata": {
    "id": "15168d78-4c0d-4503-9cab-18166e2aec31"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11a27a34-ba86-426a-98be-4f74762821be",
   "metadata": {
    "id": "11a27a34-ba86-426a-98be-4f74762821be"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87051086-2591-4c23-b906-fca6cdb7d243",
   "metadata": {
    "id": "87051086-2591-4c23-b906-fca6cdb7d243"
   },
   "outputs": [],
   "source": [
    "# We create the model with a custom function\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    # Shift+TAB for documentation\n",
    "    # Glorot stands for the Xavier initialization, after Xavier Glorot\n",
    "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    # We use our custom loss function\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2e3dd4d-5c37-46a6-ab27-df1e00f6a918",
   "metadata": {
    "id": "c2e3dd4d-5c37-46a6-ab27-df1e00f6a918"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size = vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1fc5eff7-32f7-405f-b3db-a51029b2a75d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fc5eff7-32f7-405f-b3db-a51029b2a75d",
    "outputId": "4d987f5b-e227-4c42-93da-871abf5a2ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (128, None, 64)           5376      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (128, None, 1026)         3361176   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, None, 84)           86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c5d4c0-2b0d-4d9f-99a6-8a6a1bb59f2a",
   "metadata": {
    "id": "29c5d4c0-2b0d-4d9f-99a6-8a6a1bb59f2a"
   },
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda885bd-eef0-41d1-aaa3-e37a131db103",
   "metadata": {
    "id": "eda885bd-eef0-41d1-aaa3-e37a131db103"
   },
   "source": [
    "**Important note:** we see that our RNN model has around 3.5M parameters, which is not viable to train on a laptop. Instead, a computer with powerful GPUs is required. Therefore, **Google Colab** is used in the course.\n",
    "\n",
    "Follow these instructions:\n",
    "- Open [Google Colab](https://colab.research.google.com/) and log in\n",
    "- On the same URL, choose to upload this file\n",
    "- On the Colab notebook: left menu bar: files (folder icon) > upload dataset (`shakespeare.txt`) and any other file necessary (clicking on icon or just drag & drop; for instance the pre-trained model `shakespeare_gen.h5`)\n",
    "\n",
    "Some notes on Google Colab:\n",
    "- Note that Markdown images can be rendered if these are in gDrive and have a shareable link\n",
    "- Have a look at the Colab examples: Welcome, Charts, Moounting Drives, etc.\n",
    "- Note that the workspace is erased once we exit!\n",
    "- Note that the workspace is not in our gDrive, but apparently a linux container is started; however, our notebooks are saved on our gDrive when we exit - but only our notebooks!\n",
    "- Check that we are using TF version >= 2 (see beginning of the notebook)\n",
    "- We can run all cells as always: Runtime > Run all\n",
    "- Tools > Settings > Editor: TABs should be 4 spaces!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VqNVmJs-sioR",
   "metadata": {
    "id": "VqNVmJs-sioR"
   },
   "source": [
    "### Some Preliminary Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "WBc2C2kLhw1B",
   "metadata": {
    "id": "WBc2C2kLhw1B"
   },
   "outputs": [],
   "source": [
    "# We get a single batch, feed it to our model and get the output\n",
    "# Since th emodel has not been trained yet, the output should be rubish\n",
    "# Recall input and target have 128 sequences of length 120, each shifted 1 character\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1fObyUhe2Ez",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1fObyUhe2Ez",
    "outputId": "5d80f2fc-69ff-4f27-a560-6e701fdcf4a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 120, 84])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions are 128 sequences of length 120\n",
    "# For each of the 120 characters in a sequence, we have 84 probablities\n",
    "# associated to each of the 84 characters/vocabs.\n",
    "# These probablities are also called logits.\n",
    "example_batch_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "pObfr2qHt_-i",
   "metadata": {
    "id": "pObfr2qHt_-i"
   },
   "outputs": [],
   "source": [
    "# We take the first sequence of the 128\n",
    "# and convert its 120 characters to categorical values ranging 0-83;\n",
    "# num_samples=1 means from all 84 we take the largest\n",
    "# categorical() returns the index of the largest vocab/char\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "V8wj-fVhuIM7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8wj-fVhuIM7",
    "outputId": "df9c4466-4dea-48b6-ca63-f7c38c8704ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5475, shape=(120, 1), dtype=int64, numpy=\n",
       "array([[83],\n",
       "       [25],\n",
       "       [39],\n",
       "       [53],\n",
       "       [ 8],\n",
       "       [ 6],\n",
       "       [17],\n",
       "       [61],\n",
       "       [54],\n",
       "       [42],\n",
       "       [36],\n",
       "       [28],\n",
       "       [55],\n",
       "       [53],\n",
       "       [13],\n",
       "       [72],\n",
       "       [35],\n",
       "       [10],\n",
       "       [ 2],\n",
       "       [27],\n",
       "       [59],\n",
       "       [73],\n",
       "       [27],\n",
       "       [44],\n",
       "       [55],\n",
       "       [46],\n",
       "       [63],\n",
       "       [76],\n",
       "       [41],\n",
       "       [44],\n",
       "       [ 3],\n",
       "       [56],\n",
       "       [73],\n",
       "       [ 7],\n",
       "       [77],\n",
       "       [58],\n",
       "       [59],\n",
       "       [34],\n",
       "       [41],\n",
       "       [73],\n",
       "       [22],\n",
       "       [ 4],\n",
       "       [35],\n",
       "       [45],\n",
       "       [54],\n",
       "       [61],\n",
       "       [27],\n",
       "       [43],\n",
       "       [65],\n",
       "       [33],\n",
       "       [76],\n",
       "       [47],\n",
       "       [56],\n",
       "       [ 4],\n",
       "       [ 8],\n",
       "       [83],\n",
       "       [58],\n",
       "       [42],\n",
       "       [52],\n",
       "       [ 2],\n",
       "       [68],\n",
       "       [68],\n",
       "       [60],\n",
       "       [16],\n",
       "       [54],\n",
       "       [27],\n",
       "       [ 0],\n",
       "       [58],\n",
       "       [ 2],\n",
       "       [53],\n",
       "       [54],\n",
       "       [18],\n",
       "       [68],\n",
       "       [81],\n",
       "       [71],\n",
       "       [65],\n",
       "       [69],\n",
       "       [80],\n",
       "       [54],\n",
       "       [76],\n",
       "       [67],\n",
       "       [59],\n",
       "       [62],\n",
       "       [64],\n",
       "       [39],\n",
       "       [29],\n",
       "       [ 8],\n",
       "       [80],\n",
       "       [41],\n",
       "       [55],\n",
       "       [58],\n",
       "       [21],\n",
       "       [ 5],\n",
       "       [76],\n",
       "       [10],\n",
       "       [18],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [67],\n",
       "       [11],\n",
       "       [80],\n",
       "       [57],\n",
       "       [67],\n",
       "       [67],\n",
       "       [61],\n",
       "       [79],\n",
       "       [46],\n",
       "       [ 2],\n",
       "       [43],\n",
       "       [78],\n",
       "       [69],\n",
       "       [22],\n",
       "       [19],\n",
       "       [23],\n",
       "       [49],\n",
       "       [45],\n",
       "       [29],\n",
       "       [50],\n",
       "       [70],\n",
       "       [14]])>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ykWNt1KWviF7",
   "metadata": {
    "id": "ykWNt1KWviF7"
   },
   "outputs": [],
   "source": [
    "# We reshape the sampled_indices and convert them to a numpy array\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "Kvy1jaulv3Dz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kvy1jaulv3Dz",
    "outputId": "0aa615c1-c30f-4159-a931-b7d6152463f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83, 25, 39, 53,  8,  6, 17, 61, 54, 42, 36, 28, 55, 53, 13, 72, 35,\n",
       "       10,  2, 27, 59, 73, 27, 44, 55, 46, 63, 76, 41, 44,  3, 56, 73,  7,\n",
       "       77, 58, 59, 34, 41, 73, 22,  4, 35, 45, 54, 61, 27, 43, 65, 33, 76,\n",
       "       47, 56,  4,  8, 83, 58, 42, 52,  2, 68, 68, 60, 16, 54, 27,  0, 58,\n",
       "        2, 53, 54, 18, 68, 81, 71, 65, 69, 80, 54, 76, 67, 59, 62, 64, 39,\n",
       "       29,  8, 80, 41, 55, 58, 21,  5, 76, 10, 18,  5,  8, 67, 11, 80, 57,\n",
       "       67, 67, 61, 79, 46,  2, 43, 78, 69, 22, 19, 23, 49, 45, 29, 50, 70,\n",
       "       14])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "jpj5JZ6Av44-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpj5JZ6Av44-",
    "outputId": "4c8cca32-38f7-4057-b8d3-5629c19485f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['}', '?', 'N', ']', ',', '(', '6', 'f', '_', 'Q', 'K', 'C', '`',\n",
       "       ']', '2', 'q', 'J', '.', '!', 'B', 'd', 'r', 'B', 'S', '`', 'U',\n",
       "       'h', 'u', 'P', 'S', '\"', 'a', 'r', ')', 'v', 'c', 'd', 'I', 'P',\n",
       "       'r', ';', '&', 'J', 'T', '_', 'f', 'B', 'R', 'j', 'H', 'u', 'V',\n",
       "       'a', '&', ',', '}', 'c', 'Q', '[', '!', 'm', 'm', 'e', '5', '_',\n",
       "       'B', '\\n', 'c', '!', ']', '_', '7', 'm', 'z', 'p', 'j', 'n', 'y',\n",
       "       '_', 'u', 'l', 'd', 'g', 'i', 'N', 'D', ',', 'y', 'P', '`', 'c',\n",
       "       ':', \"'\", 'u', '.', '7', \"'\", ',', 'l', '0', 'y', 'b', 'l', 'l',\n",
       "       'f', 'x', 'U', '!', 'R', 'w', 'n', ';', '8', '<', 'X', 'T', 'D',\n",
       "       'Y', 'o', '3'], dtype='<U1')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We convert it to chars\n",
    "# It's a set of random characters\n",
    "ind_to_char[sampled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oTNFFiu33Ct-",
   "metadata": {
    "id": "oTNFFiu33Ct-"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "X-UBL61CwIFq",
   "metadata": {
    "id": "X-UBL61CwIFq"
   },
   "outputs": [],
   "source": [
    "# We need at least 30 epochs\n",
    "# This models and dataset require approximately 1 min/epoch to train\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135NQXdQ3GIw",
   "metadata": {
    "id": "135NQXdQ3GIw"
   },
   "outputs": [],
   "source": [
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAfc65WF6Hg-",
   "metadata": {
    "id": "CAfc65WF6Hg-"
   },
   "outputs": [],
   "source": [
    "# Save our model\n",
    "model.save('my_shakespeare_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wCKAXqUe5mEC",
   "metadata": {
    "id": "wCKAXqUe5mEC"
   },
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CHOlHkbG5tuH",
   "metadata": {
    "id": "CHOlHkbG5tuH"
   },
   "source": [
    "We can wait until the training finishes or interrupt the training load the custom model provided in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "XQbsdRm63JUY",
   "metadata": {
    "id": "XQbsdRm63JUY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "febPx9iC5iiH",
   "metadata": {
    "id": "febPx9iC5iiH"
   },
   "outputs": [],
   "source": [
    "# Loading the model: first we create model and the we load the weights\n",
    "# Note that the batch size is now 1, we don't pass batches of 128 sequences!\n",
    "# Therefore, be need to rebuild it\n",
    "model = create_model(vocab_size = vocab_size,\n",
    "                     embed_dim=embed_dim,\n",
    "                     rnn_neurons=rnn_neurons,\n",
    "                     batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "vQbfuRJb6nct",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "vQbfuRJb6nct",
    "outputId": "4c4807c9-1802-4a8c-bac3-3ef57edafaf9"
   },
   "outputs": [],
   "source": [
    "#model.load_weights('./my_shakespeare_model.h5')\n",
    "model.load_weights('./shakespeare_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46tPzKDz61Wf",
   "metadata": {
    "id": "46tPzKDz61Wf"
   },
   "outputs": [],
   "source": [
    "# Then, we build it by passing the input shape\n",
    "# Note that now it is different: we don't pass batches of 128 sequences!\n",
    "model.build(tf.TensorShape([1,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "DH8G5iVQ7iQu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DH8G5iVQ7iQu",
    "outputId": "31cda612-bf85-4068-ffbc-b69e844973fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 64)             5376      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1026)           3361176   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 84)             86268     \n",
      "=================================================================\n",
      "Total params: 3,452,820\n",
      "Trainable params: 3,452,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X4T_Y9lg7m76",
   "metadata": {
    "id": "X4T_Y9lg7m76"
   },
   "source": [
    "### Text Generator Function\n",
    "\n",
    "We carry out the inference inside a function `generate_text`, which receives the input text and the expected size of the returned text.\n",
    "\n",
    "Note that the instructor uses a variable `temp` to scale the probabilites of each vocab/char in a prediction; `temp` should regulate the weirdness of the predictions. However, as I write in the code comments below, it seems useless to me as it is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28170a6-d90e-458c-ae7f-bddb38499459",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** See the section below (Single Character Predictions) to better understand what is happening in the function. Sizes/Shapes of the tensors are not that trivial for me without trying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "Qd_X0Mc-7J_i",
   "metadata": {
    "id": "Qd_X0Mc-7J_i"
   },
   "outputs": [],
   "source": [
    "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
    "    # Expected sequence of characters\n",
    "    num_generate = gen_size\n",
    "    # Convert chars to indices:\n",
    "    # an array consisting of all the chars passed as seed is built\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    # Re-shape to have more dimensions\n",
    "    input_eval = tf.expand_dims(input_eval,0)\n",
    "    text_generated = []\n",
    "    temperature = temp\n",
    "    # Rest model states\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        # Forward pass:\n",
    "        # The model predicts an array of char-indices of the same length as input_eval\n",
    "        # input_eval is the whole seed at the beginning,\n",
    "        # then just a character/symbol\n",
    "        predictions = model(input_eval)\n",
    "        # Reverse of expand_dims\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        # Scaling all vocab predictions and choosing max\n",
    "        # is the same as directly choosing max\n",
    "        # I think temp is useless here?\n",
    "        predictions = predictions/temperature\n",
    "        # Get the index of the LAST predicted character\n",
    "        # Therefore, if len(input_eval)>1, only the last predicted char/symbol is taken\n",
    "        predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
    "        # Next input is current output!\n",
    "        input_eval = tf.expand_dims([predicted_id],0)\n",
    "        # Concatenate one by one the char/symbol predicted in each step of range(num_generate)\n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "    # Return final text of length num_generate\n",
    "    return (start_seed+\"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cXTQkaGt_j9C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "cXTQkaGt_j9C",
    "outputId": "6659ad24-f9d1-41ce-8c22-0684b45df1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEOCTINE]  O sleep, thy slips nor words, untung'd in this\n",
      "    extravagaing; for the rebels' HERALD green with read\n",
      "\n",
      "  KING JOHN. My lord-\n",
      "  AARON. [Wounds] I have forswear my weapon.\n",
      "  Marget EELEL. I hope, I shall remember so far afflicted  \n",
      "    your father. Who was it bad she? Without my sword.\n",
      "  HERMIONE. It stuck me.\n",
      "  CLOWN. Ay, sir; but, my lords,\n",
      "    Desire to piecis; nor ed down\n",
      "    Moves not. Then there lies d before us seeming one anon. Captain Fluelles of fire,\n",
      "    Allowing him to the Mount; which makes from heaven\n",
      "    And razs a tear.\n",
      "  DEMETRIUS. What my men day lie scars, provoke no Roman babe! There were surpris'd\n",
      "          and Doot I prosperous on his own.\n",
      "  Leon. Could she beat?\n",
      "  Vere. Catescand up, my lord, I have no eyes.\n",
      "    There she's a trick, thy father's top-O. What? Marcellus!\n",
      "  AUTOLYCUS. Are you commit such a friend? Courage me command thou then defend\n",
      "    That which loy sounsing you whose judgment sure\n",
      "    Will cure not first, t a coronets. Fairest men to you\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,'ROME',gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f125157-8fd0-4e08-94fa-bdf4b67f4412",
   "metadata": {},
   "source": [
    "### Single Character Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b5f59325-f74e-43f7-8c7c-6d60d6238136",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f61f76e1-fc79-4f6d-863b-ad346d620eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_eval = [char_to_ind[s] for s in word]\n",
    "input_eval = tf.expand_dims(input_eval,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9f48d66a-2a71-49fb-b4d8-29f23ca40a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass the complete word to the model\n",
    "predictions = model(input_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a14283ee-d018-4fd2-8f77-7e2d638ebdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 84])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model returns the same number of characters as the input word\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "164788aa-06ab-4adf-87ac-cc813a3e5ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the first dimension\n",
    "predictions = tf.squeeze(predictions,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "22a2b235-550e-4040-9552-3b87f7a54d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 84])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1578e8bc-900a-40d7-880b-5db2e174fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical indices are taken for each character\n",
    "predicted_id = tf.random.categorical(predictions,num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "20088324-764a-4948-a370-a7b4ab03121f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=513134, shape=(5, 1), dtype=int64, numpy=\n",
       "array([[ 1],\n",
       "       [ 1],\n",
       "       [80],\n",
       "       [10],\n",
       "       [78]])>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "059da4a4-92a8-4595-a661-3f55cb5df2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The last character is taken\n",
    "predicted_id[-1,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec708e-7911-463e-a69f-20d8c23f4e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "278c398b-12e9-4c73-9690-bdd72e5eaf0d",
    "bbf4a0da-4d5b-4447-a56f-f4d2e374c546"
   ],
   "name": "19_08_1_Keras_NLP_TextGeneration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
