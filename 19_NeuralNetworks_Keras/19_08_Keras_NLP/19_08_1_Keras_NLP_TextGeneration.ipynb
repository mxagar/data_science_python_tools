{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad9043-625d-4c03-bf8c-e4c024c50f28",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94865b4-2a5b-444b-89bd-a9e418bb44e7",
   "metadata": {},
   "source": [
    "This section is based on the following blog post by Andrej Karpathy:\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "The code used by Karpathy for the article is on Github:\n",
    "\n",
    "[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)\n",
    "\n",
    "Basically, it is a **character-level language model**; astonishingly, the network will learn to create text, even being trained on a character level!\n",
    "\n",
    "The basic idea is show on th efollowing picture from Karpathy's blog post:\n",
    "\n",
    "![Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`](pics/charseq_karpathy.jpeg)\n",
    "*Karpathy: An example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). The vocabulary is `[h,e,l,o]`.*\n",
    "\n",
    "The current notebook is about creating a simplified project, skmilar to the one described in the article, with the following goal: Given a sequence of characters, predict the same sequence shifted one character: e.g., `[h,e,l,l] -> [e,l,l,o]`.\n",
    "\n",
    "Some points to consider:\n",
    "- We are going to use the complete works by Shakespeare for training. The reansons are: (1) we have more than one million characters in the text and (2) the text is very well structured.\n",
    "- We are going to create a one-hot encoding for the alphabet characters and punctuation; then, we are going to use an embedding to compress those one-hot vectors.\n",
    "\n",
    "Steps followed:\n",
    "1. Load text/data; a large dataset with millions of characters is required\n",
    "2. Text processing and vectorization: integers assigned to letterns and symbols (e.g., punctuation)\n",
    "3. Create batches: create long enough sequences to learn relationships, but not too long to avoid noise\n",
    "4. Crate the model: we'll have 3 layers\n",
    "    - Embedding layer: one-hot encoding vectors are compressed to a smaller space of fixed size (dimensions)\n",
    "    - GRU layer: a simplified version of LSTM units (i.e., with fewer parameters), which leads to better results (see RNN folder: `../19_07_Keras_RNN`)\n",
    "    - Dense layer: probabilities per character\n",
    "5. Train the model\n",
    "6. Inference\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "A nice description of what embeddings are is given in this video on the DotCSV Youtube channel:\n",
    "\n",
    "[INTRO al Natural Language Processing (NLP) #2 - ¿Qué es un EMBEDDING?](https://www.youtube.com/watch?v=RkYuH_K7Fx4)\n",
    "\n",
    "Embeddings are not exclusive to language, but are commonly used in it, thanks to approaches like `word2Vec`, published in\n",
    "\n",
    "\"Efficient Estimation of Word Representations in Vector Space\", Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. 2013, Google.\n",
    "\n",
    "The idea is that, first, **we create a one-hot encoding to represent our vocabulary** in order to start working on the text; the size of the one-hot vector is the size of the vocabulary (i.e., the number of words, say 10k). This representation has several problems, such as:\n",
    "- It is large and sparse.\n",
    "- Words that are close to each other semantically ar ethe same dinstance apart as words that should be far away.\n",
    "\n",
    "In order to solve those issues, a shallow neural net can be applied to the one-hot vectors to compressed them to a space with less dimensions (e.g., 300) but continuous values:\n",
    "\n",
    "`[0,0,0,1,0,0,0] -> [0.54, 0.01]`\n",
    "\n",
    "The nice thing is that vectors in the embedding space that are close to each other are in the reality semantically close to each other. Thus, we could start applying typical algebra operations on them, in such a way that `V(king) - V(man) + V(woman)` should be close to `V(queen)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c34f9-7bc3-48e4-be44-13d438266a6e",
   "metadata": {},
   "source": [
    "## 1. Load Text/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9922bba3-b9e7-4051-b38c-90505a93f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f679a73a-b7c3-4dc7-bf39-4006df898347",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./shakespeare.txt','r') as f:\n",
    "    #lines = f.readlines() \n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67888193-4e6e-453d-8bae-c8d18d7372b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose mi\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The text has symbols in it, such as \\n\n",
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d942771f-c20c-4fb6-80f5-9dfcd5ce5549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houldst not abhor my state.\n",
      "    If thy unworthiness raised love in me,\n",
      "    More worthy I to be beloved of thee.\n",
      "\n",
      "\n",
      "                     151\n",
      "  Love is too young to know what conscience is,  \n",
      "  Yet who knows not conscience is born of love?\n",
      "  Then gentle cheater urge not my amiss,\n",
      "  Lest guilty of my faults thy sweet self prove.\n",
      "  For thou betraying me, I do betray\n",
      "  My nobler part to my gross body's \n"
     ]
    }
   ],
   "source": [
    "# If we print it, the symbols are interpreted\n",
    "print(text[100100:100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbafe5-66c5-4c3b-a29f-c22006e1a65d",
   "metadata": {},
   "source": [
    "## 2. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "945d39bc-0d73-410a-8d29-ee6d0e34f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a set with all characters and symbols\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a580d1f-0ad8-4093-b372-58509b2b99eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8ff7492-7d1b-4e22-a0e8-906c37582580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of characters/symbols we have - important for the final dense layer\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cc412af-60f6-4cee-902c-9152bee0f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, '&')\n",
      "(5, \"'\")\n",
      "(6, '(')\n",
      "(7, ')')\n",
      "(8, ',')\n",
      "(9, '-')\n",
      "(10, '.')\n",
      "(11, '0')\n",
      "(12, '1')\n",
      "(13, '2')\n",
      "(14, '3')\n",
      "(15, '4')\n",
      "(16, '5')\n",
      "(17, '6')\n",
      "(18, '7')\n",
      "(19, '8')\n",
      "(20, '9')\n",
      "(21, ':')\n",
      "(22, ';')\n",
      "(23, '<')\n",
      "(24, '>')\n",
      "(25, '?')\n",
      "(26, 'A')\n",
      "(27, 'B')\n",
      "(28, 'C')\n",
      "(29, 'D')\n",
      "(30, 'E')\n",
      "(31, 'F')\n",
      "(32, 'G')\n",
      "(33, 'H')\n",
      "(34, 'I')\n",
      "(35, 'J')\n",
      "(36, 'K')\n",
      "(37, 'L')\n",
      "(38, 'M')\n",
      "(39, 'N')\n",
      "(40, 'O')\n",
      "(41, 'P')\n",
      "(42, 'Q')\n",
      "(43, 'R')\n",
      "(44, 'S')\n",
      "(45, 'T')\n",
      "(46, 'U')\n",
      "(47, 'V')\n",
      "(48, 'W')\n",
      "(49, 'X')\n",
      "(50, 'Y')\n",
      "(51, 'Z')\n",
      "(52, '[')\n",
      "(53, ']')\n",
      "(54, '_')\n",
      "(55, '`')\n",
      "(56, 'a')\n",
      "(57, 'b')\n",
      "(58, 'c')\n",
      "(59, 'd')\n",
      "(60, 'e')\n",
      "(61, 'f')\n",
      "(62, 'g')\n",
      "(63, 'h')\n",
      "(64, 'i')\n",
      "(65, 'j')\n",
      "(66, 'k')\n",
      "(67, 'l')\n",
      "(68, 'm')\n",
      "(69, 'n')\n",
      "(70, 'o')\n",
      "(71, 'p')\n",
      "(72, 'q')\n",
      "(73, 'r')\n",
      "(74, 's')\n",
      "(75, 't')\n",
      "(76, 'u')\n",
      "(77, 'v')\n",
      "(78, 'w')\n",
      "(79, 'x')\n",
      "(80, 'y')\n",
      "(81, 'z')\n",
      "(82, '|')\n",
      "(83, '}')\n"
     ]
    }
   ],
   "source": [
    "# Now, we need to bidirectionally associate each character in the vocabulary\n",
    "# with a number (related to one-hot encoding):\n",
    "# character <-> number\n",
    "# We can do that with enumerate and dictionaries\n",
    "for pair in enumerate(vocab):\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af337315-5c98-4270-b363-c7d0ac6992ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following that, we create a dictionary with comprehension\n",
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8059957c-579c-4ffc-80ee-064b9a8fed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional association\n",
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b4f63b5-ab18-4c79-bc7f-8c1a3caf1fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e0e2caf-960d-47ab-80ca-fcfab0bf2433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_to_char[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3cc0a6e8-0e9e-490b-9878-fbd5700c07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, with those two vectors, we can encodde our text!\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc10d3b2-bc2c-4708-a7d1-9cd62b539ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5445609,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We check that we have several millions of characters (necessary for good enough results)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a7230-cf54-45bd-bcbc-dbb862b9af83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
