{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b055a9c0-b4d4-437b-b815-0af26d2ba5a6",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47d5ef-2c02-41df-99ff-74d3fe34d138",
   "metadata": {},
   "source": [
    "While CNNs work with filters or kernels that are applied in local patches of our data, RNNs work with sequences of data in which the order is important.\n",
    "We have a sequence of elements (e.g., a time series or a sentence of words) and the goal of a RNN is to predict the next element(s) given the sequence, i.e., we want to forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff44dc-39e6-4941-9b4a-e03a3a03135a",
   "metadata": {},
   "source": [
    "### (Very) Basic Theory\n",
    "\n",
    "Examples of sequences: time series (e.g., sales), sentences, audio, car trajectories, music, etc.\n",
    "\n",
    "Note that the **order or history is important; actually that's one of the tings is supposed to be understood/learnt.**\n",
    "From that intuitive idea, we can derive how a recurrent neuron of a RNN is built: **previous outputs are fed back as inputs!**\n",
    "\n",
    "Some properties:\n",
    "- Recurrent neurons are also called *memory cells*\n",
    "- RNNs are very flexible in terms of how outputs are converted into inputs\n",
    "    - We can pass single vector of values\n",
    "    - We can pass a sequence\n",
    "    - We can pass entire layer outputs as inputs again to the same layer\n",
    "- Typical architectures\n",
    "    - **Sequence-to-sequence** (aka. *many to many*): we pass a sequence and exppect a sequence. For example, we could train a chatbot with Q-A sequences.\n",
    "    - **Sequence-to-vector** (aka. *many to one*): we pass a sequence and expect an element. For example, we can use that architecture to generate text.\n",
    "    - **Vector-to-sequence** (aka. *one to many*): for instance, given a word, predict the next 5.\n",
    "- Major drawbacks of RNNs:\n",
    "    1. The **vanishing gradient**, as in all other ANNs.\n",
    "    2. We pass the previous output as input, so **we forget older elements**. It would be great to keep track of longer history, not only the short-term one. The **Long Short Term Memory Units (LSTMs)** address that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0566aa-6b6d-41c0-ba3a-24db8e7416bc",
   "metadata": {},
   "source": [
    "### Exploding and Vanishing Gradients\n",
    "\n",
    "Complex data usually requires more complex architectures that consist of more hidden layers.\n",
    "\n",
    "When we train, we iteratively compute the derivative of the error (i.e., the difference between the expected and predicted output) with respect to the weights.\n",
    "That error derivative is backpropagated to the neuron weights using the rule of chain and each weight is updated so that the error becomes smaller.\n",
    "**Since we use the rule of chain for computing the derivative, we have chained multiplications of values; if the multiplied values are small, the resulting gradient becomes very small (it vanishes), and it becomes gigant if the values are big (it explodes).**\n",
    "That is not desired, since we cannot accurately control how to change the weights: usually, too small insignificant changes are computed due to the vanishing gradient, especially closer to the input layers. Thus, basic patterns that should be detected in the first layers are not learnt!\n",
    "\n",
    "The vanishing gradient is particularly string in RNNs.\n",
    "\n",
    "Some points to take into account regarding the the **vanishing gradient**:\n",
    "- The **sigmoid activation** makes the vanishing gradient worse: we squeeze/map the signals to the [0,1] region, but the slope (i.e., derivative) of the activation function is very small!\n",
    "    - Better, use the ReLu (rectified linear unit): max(x,0).\n",
    "    - Other options: leaky ReLu, ELU, etc.\n",
    "- **Batch normalization** avoids also the vanishing gradient: batches of samples are normalized using the mean and the standard deviation of the batch.\n",
    "- **Xavier initialization**: a method for choosing different initial weight values. Basically, initial values are set to belong to a uniform distribution scaled by `sqrt(6)/sqrt(in + out connections)`; it improves alleviates diminishing gradients.\n",
    "- **Gradient clipping**: we clip a gradient when it reaches a given limit value; a drity trick that seems to work sometimes.\n",
    "- **LSTM units** also reduce the phenomenon of the vanishing gradient, apart from bringing some advantages to reguar recursive neurons. However, exploding gradients can still occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdfbd2-21e2-4cab-b67d-a7772393b47b",
   "metadata": {},
   "source": [
    "### LSTM and GRU: Long Short Term Memory Units and Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53fdeec-afa3-41dd-986f-5345585e2913",
   "metadata": {},
   "source": [
    "A simple recursive neuron takes its previous output $h(t-1)$ as additional input to the regular input $x(t)$.\n",
    "The output $h(t)$ is usually computed by multiplying the weights to the concatenated inputs and applying the hyperbilic tangent as activation function:\n",
    "\n",
    "$h(t) = \\mathrm{tanh}(w\\cdot[h(t-1)^{T},x(t)^{T}]^{T} + b)$\n",
    "\n",
    "Recall that since the structures of RNNs are quite flexible, we could do that per neuron or per layer.\n",
    "\n",
    "A **long short term memory** neuron is more sophisticated, as it consits of three entries and two exits:\n",
    "\n",
    "- As recurrent outputs, we have short-term ($h$) and long-term ($c$) memory signals; their output value at time $t-1$ is also input at time $t$\n",
    "- The short-term output $h$ is also the output that goes to the next neuron/layer\n",
    "- Additionally we have the regular input $x$ coming from previous neurons/layers\n",
    "\n",
    "Internally, several gates (paths) are distinguished (forget, input/update, output).\n",
    "The general idea is that irrelevant signals are forgotten, while relevant data is preserved;\n",
    "that is achieved by appliying sigmoid and tanh activations to the values and performing summations and mutiplications between them.\n",
    "\n",
    "The following figure from the Wikipedia displays the internal operations:\n",
    "\n",
    "![LSTM cell - source: Wikipedia](pics/LSTM_Cell.png)\n",
    "\n",
    "Image Source: [Wikipedia article on LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)\n",
    "\n",
    "There are several variations of that LSTM unit, such as\n",
    "- **Peephole** units, which concatenate internal values\n",
    "- **Gated Recurrent Units (GRUs)**, which combine forget an input gates to a single upddate gate, simplifying the unit\n",
    "\n",
    "Although LSTMs are the default units used in RNNs, GRUs are getting more attention and are being deploying more and more lately.\n",
    "\n",
    "![GRU cell - source: Wikipedia](pics/GRU_Cell.png)\n",
    "\n",
    "Image Source: [Wikipedia article on GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ff28d5-1516-4fd8-985b-e308a042c0f4",
   "metadata": {},
   "source": [
    "### Interesting Links\n",
    "\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Exploring LSTMs, by Edwin Chen](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "- [When to use GRU over LSTM? Stackexchange](https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c10ebd-476a-46a5-b5e3-92c99256537a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
