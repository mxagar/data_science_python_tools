{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ef6620-0dc1-4c24-abf5-6037364c6c41",
   "metadata": {},
   "source": [
    "# DQN on Custom Environments - Snake Game: Training AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b709b-222d-403c-884e-4d3a19dd1e1d",
   "metadata": {},
   "source": [
    "This notebook trains a DQN agent using Keras-RL2 to play our custom game Snake. Follow the guide (Section 7) `../ReinforcementLearning_Guide.md` for more information.\n",
    "\n",
    "Prior to executing this notebook, the previous two and the game folder structure `snake/` must have been correctly created.\n",
    "\n",
    "This nobetook is basically a modification of the notebook `04_2_DQN_Images_Keras_RL2_Breakout.ipynb`; the main modification is that we create an instance of our game environment Snake, instead of Breakout.\n",
    "\n",
    "Overview:\n",
    "1. Imports\n",
    "2. Environment Setup\n",
    "3. Image Processing\n",
    "4. Network Model\n",
    "5. Agent\n",
    "6. Training & Storing\n",
    "7. Test & Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4999836-ce10-4747-867d-2222e08a007b",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "180dc9a7-61b6-4b6b-b952-7192ff180bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Keras-RL2\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02075ed-1f6e-4502-8138-bf19eeeeecfa",
   "metadata": {},
   "source": [
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ec7f3f7-afba-4fee-96a3-640bca3e75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom environment\n",
    "env = gym.make(\"snake:snake-v0\")\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1703f-6829-4bbe-87b7-79c977432045",
   "metadata": {},
   "source": [
    "## 3. Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51db65fa-6592-4c96-96f1-81b6c52c380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though the game is 200x200,\n",
    "# we need to downsample it to train in feasible times\n",
    "IMG_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ad601c-04b0-469e-a104-bb0457c9a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # numpy -> PIL\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(IMG_SHAPE)\n",
    "        # RGB -> grayscale\n",
    "        img = img.convert(\"L\")\n",
    "        # PIL -> numpy\n",
    "        img = np.array(img)\n",
    "        return img.astype('uint8') # compress\n",
    "    \n",
    "    def process_state_batch(self, batch):\n",
    "        # [0,255] -> [0, 1], for training the nerual network\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b912c-a11a-479c-9e36-fee0fdc6c1bd",
   "metadata": {},
   "source": [
    "## 4. Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c2494-f161-43af-80e7-732c0ae27762",
   "metadata": {},
   "source": [
    "The model greatly varies on the type of custom game/environment. Read papers of similar environments to figure out which architectures to define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e829e71e-0055-49e4-a31e-50f21d5b31b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (WINDOW_LENGTH, IMG_SHAPE[0], IMG_SHAPE[1])\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a37984c-7594-431e-93aa-59e58bd98153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4),kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c5a01-9ecf-4c0d-8c06-4c5c435ee9e1",
   "metadata": {},
   "source": [
    "## 5. Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a040c0-99be-4dc6-b635-073489e81669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a120d92-7cf8-4a6c-9826-a63637ca140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image processor\n",
    "processor = ImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc1a9b38-6b37-4c47-8b82-eb7d873ac2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95f7a7bb-a425-465e-9953-14b93e8d6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=.99,\n",
    "               target_model_update=10000,\n",
    "               train_interval=4,\n",
    "               delta_clip=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d21ee5a4-ed9b-464c-b321-77d7605fe5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f8222df-4e50-4bff-b48d-c46556960235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for trained weigts + checkpoints\n",
    "weights_filename = 'test_dqn_snake_weights.h5f'\n",
    "checkpoint_weights_filename = 'test_dqn_' + \"snake\" + '_weights_{step}.h5f'\n",
    "checkpoint_callback = ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af530e-715c-4839-8008-e31f5664f4cd",
   "metadata": {},
   "source": [
    "## 6. Training & Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc0df70-147e-4f42-af1e-1b5ade6081ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    14/100000 [..............................] - ETA: 6:26 - reward: 0.0000e+00   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikel\\.conda\\envs\\ds\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 1543s 15ms/step - reward: -0.0205\n",
      "2296 episodes - episode_reward: -0.895 [-1.000, 2.000] - loss: 0.003 - mae: 0.108 - mean_q: 0.138 - mean_eps: 0.932 - score: 0.081\n",
      "\n",
      "Interval 2 (100000 steps performed)\n",
      " 21001/100000 [=====>........................] - ETA: 36:46 - reward: -0.0197done, took 2130.279 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train and save checkpoint weights\n",
    "# Note that 1.5M steps takes many hours to train!\n",
    "# We can stop it and load the weights provided in teh course\n",
    "dqn.fit(env,\n",
    "        nb_steps=1500000,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        log_interval=100000,\n",
    "        visualize=False)\n",
    "\n",
    "# Save final weights\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b90bbb-3738-4e90-8559-ea634d3d7968",
   "metadata": {},
   "source": [
    "## 7. Test & Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e20ef1-e6e7-4263-bd01-81509dc5424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "model.load_weights(\"./snake_weights/dqn_snake_weights_1200000.h5f\")\n",
    "# Replay buffer\n",
    "memory = SequentialMemory(limit=1000000,\n",
    "                          window_length=WINDOW_LENGTH)\n",
    "# Updated Policy\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=100000)\n",
    "# Image Processor\n",
    "processor = ImageProcessor()\n",
    "# Initialize the DQNAgent with the new model and updated policy and compile it\n",
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=.99,\n",
    "               target_model_update=10000)\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cddd8da-a12d-46c3-9633-8f433caa370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sleep, otherwise the snake is going to move too fast!\n",
    "env.sleep = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb57d262-80a9-42bc-a2d1-9d1285b5b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 6.000, steps: 86\n",
      "Episode 2: reward: 7.000, steps: 107\n",
      "Episode 3: reward: 7.000, steps: 106\n",
      "Episode 4: reward: 6.000, steps: 132\n",
      "Episode 5: reward: 5.000, steps: 139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x194e8e5fc08>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This loop does not seem to work on my computer...\n",
    "# ... but it seems to be a Windows issue...??\n",
    "# However, the reward seems not to be that bad??\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3543c1-4d96-40a9-be71-120a50796164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
