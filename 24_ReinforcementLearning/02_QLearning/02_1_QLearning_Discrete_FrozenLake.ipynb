{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f501400-522f-4ce0-a88e-56f7b47d332d",
   "metadata": {},
   "source": [
    "# Q-Learning: Discrete Example with FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76024045-2569-42b2-9752-07fd213f2814",
   "metadata": {},
   "source": [
    "This notebook implements the Q-Learning algorithm for the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) game.\n",
    "See `../ReinforcementLearning_Guide.md` for theory and intuition.\n",
    "\n",
    "According to the OpenAI environment page of FrozenLake: \"The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\"\n",
    "\n",
    "The surface is described using a grid:\n",
    "\n",
    "    SFFF       (S: starting point, safe)\n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole, fall to your doom)\n",
    "    HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "The game episode ends when the agent reaches the goal or it falls in a hole. You receive a reward of 1 if you reach the goal, zero otherwise.\n",
    "\n",
    "We are going to disable the slippery property; otherwise taken actions are not carried out necessarily, and the learning process takes longer. That change is after this post:\n",
    "\n",
    "[https://github.com/openai/gym/issues/565](https://github.com/openai/gym/issues/565)\n",
    "\n",
    "Overview of sections:\n",
    "\n",
    "1. Basic setup of FrozenLake\n",
    "2. Q-Learning Table Initialization & Hyperparameters\n",
    "3. Q-Learning Table Update Functions\n",
    "4. Training Loop\n",
    "5. Utilization and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b595d-b5ac-4381-a800-d9f0f9d2fc26",
   "metadata": {},
   "source": [
    "## 1. Basic setup of FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "accfeafb-7f04-4b74-a8b4-88cd27e99348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973fc565-0d01-487e-a744-008db5a715b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce06230b-c128-4c5d-b326-dfdae6bb6e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/tf/lib/python3.7/site-packages/gym/envs/registration.py:216: UserWarning: \u001b[33mWARN: Overriding environment FrozenLakeNotSlippery-v0\u001b[0m\n",
      "  logger.warn(\"Overriding environment {}\".format(id))\n"
     ]
    }
   ],
   "source": [
    "# In order to remove the slippery tiles, we need to create/register a new environment\n",
    "# with custom properties.\n",
    "# That can be done as explained on this link\n",
    "# https://github.com/openai/gym/issues/565\n",
    "from gym.envs.registration import register\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNotSlippery-v0', # our custom name\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv', # take the FrozenLakeEnv as the template\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False}, # changes we apply; look at Github\n",
    "        max_episode_steps=100, # default 100; 100 steps allowed in an episode\n",
    "        # the reward_threshold makes sense for games with continuous rewards\n",
    "        # such as the cart pole; but not really here\n",
    "        # we leave the default, though\n",
    "        reward_threshold=.8196, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    print('A new env can be registered only once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5329a42a-4623-4f1c-859e-fe898b4dc563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# We create an env and play in it\n",
    "# Note that the environment is completely rendered after each step/action\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.reset()\n",
    "\n",
    "for step in range(5):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)\n",
    "    #time.sleep(0.5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4fcb9fd5-0051-42a1-8349-1129168374e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clear/flush the text display we can use clear_output on Jupyter\n",
    "from IPython.display import clear_output\n",
    "# On python scripts:\n",
    "# import os\n",
    "# os.system('clear') # 'cls' ono Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "254e9467-9780-4810-8438-7c891f658b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# We create an env and play in it\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.reset()\n",
    "\n",
    "for step in range(10):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    observation,reward,done,info = env.step(action)\n",
    "    time.sleep(0.2)\n",
    "    clear_output(wait=True)\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fd360-9a48-4bd3-a20f-9126bf277728",
   "metadata": {},
   "source": [
    "## 2. Q-Learning Table Initialization & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "810102ef-755e-4c74-b3ec-c71014bbb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Table: columns x actions\n",
    "action_size = env.action_space.n # 4\n",
    "# Observation = State in this game!\n",
    "state_size = env.observation_space.n # 4*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f8bb7fe-0787-4aac-b038-d6c0e3432ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also common practice to initialize it with small and random values\n",
    "# but we leave it to 0s\n",
    "q_table = np.zeros([state_size,action_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67792556-147a-4a84-a598-29ec790227b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "463e6d80-b2a8-41a2-84fa-bf5de1b20e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585c5a7-04e5-4a36-b38e-f20a1e29b65d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfddfc24-14b9-4df1-ac2e-11937c24d057",
   "metadata": {},
   "source": [
    "Usually we get initial value estimate by reading papers. The most important hyperparameters we need to choose:\n",
    "- Alpha: learning rate; if we set it too low it is going to take vey long to converge, if too high, it's not going to learn\n",
    "- Gamma: Discount factor; factor multiplied to the future rewards (gamma^2 + gamma^3 + ...), it should be close to 1\n",
    "- Epochs = Episodes: how many times the agent plays the game, i.e. (hit done)\n",
    "- Epsilon: usually is set to 1 (complete exploration) and a decay function is defined. The decay function can be of many forms, we choose an exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c2eef43-5431-4d57-8143-0945d4a70bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad389678-c814-4b65-956c-649915c41cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f93cbe58-8acd-4f31-aaa3-0859a603553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4b0c8c87-4f5d-4a02-9e68-6320dddb2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f4a74-0320-4294-a9de-e95f03712030",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Table Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566072f-d362-45ef-8180-8799a82355ba",
   "metadata": {},
   "source": [
    "The Q-Learning update equation is\n",
    "\n",
    "$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\max_a{Q(S_{t+1},a)} - Q(S_t,A_t)]$\n",
    "\n",
    "Note that the term related to the action selection with the highest Q value is $\\max_a{Q(S_{t+1},a)}$.\n",
    "To find the balance between **exploration** (random action) vs **exploitation** (best known action = the action with the highest Q) we need to act on this term.\n",
    "For that, we use **epsilon greedy selection**: we choose a random action (exploration) depending on the epsilon value; since epsilon decays, we are going to choose less random actions with time.\n",
    "\n",
    "In summary, we need to program these steps/functions:\n",
    "- Epsilon greedy action selection\n",
    "- Q-Learning update equation\n",
    "- Epsilon (exponential) decay after each epoch/episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dcbcf2-fb90-42e1-8b80-92267f18771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon greedy action selection\n",
    "def epsilon_greedy_action_selection(epsilon,q_table,discrete_state):\n",
    "    random_number = np.random.random()\n",
    "    # Exploitation: choose action that maximizes Q\n",
    "    if random_number > epsilon:\n",
    "        # Grab state action row, with all Q values\n",
    "        state_row = q_table[discrete_state]\n",
    "        # Select the action index which has the largest Q value\n",
    "        action = np.argmax(state_row)\n",
    "    # Exploration: choose a random action\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd10f0dd-3d67-4c2a-9dbb-4e482e979d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning update equation\n",
    "# Q_next = Q_old + ALPHA*Q_error\n",
    "# Q_error = reward + GAMMA*Q_optimum - Q_old\n",
    "def compute_next_q_value(old_q_value,reward,next_optimal_q_value):\n",
    "    Q_error = reward + GAMMA*next_optimal_q_value - old_q_value\n",
    "    Q_next = old_q_value + ALPHA*Q_error\n",
    "    return Q_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8174112f-7ad7-46f1-bb5d-b5bcca8da088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon exponential decay after each episode/epoch\n",
    "def reduce_epsilon(epsilon,epoch):\n",
    "    return min_epsilon + (max_epsilon-min_epsilon)*np.exp(-decay_rate*epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c13c7b-87ef-4520-b38e-570a13abb3a4",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9e735-c59e-4430-ab02-025ae37cfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List that tracks the rewards for each episode/epoch\n",
    "rewards = []\n",
    "log_interval = 1000\n",
    "for episode in range(EPOCHS):\n",
    "    state = env.reset() # at the beginning, state = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    # Agent plays one game\n",
    "    # Done is hit if (1) we reach max steps/actions, (2) G is reached, (3) H is reached\n",
    "    while not done:\n",
    "        # Select ACTION\n",
    "        action = epsilon_greedy_action_selection(EPSILON,q_table,state)\n",
    "        # Perform step\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        # Grab (old = current) Q value: Q(s_t,a_t)\n",
    "        old_q_value = q_table[state,action]\n",
    "        # Get (next) optimal Q value from the table: Q(s_t+1,a_t+1)\n",
    "        next_optimal_q_value = np.max(q_table[new_state])\n",
    "        # Compute next Q value with Q-Learning update formula\n",
    "        next_q = compute_next_q_value(old_q_value,reward,next_optimal_q_value)\n",
    "        # Update the table\n",
    "        q_table[state,action] = next_q\n",
    "        # Track rewards\n",
    "        total_rewards = total_rewards + reward\n",
    "        # Update state for next loop\n",
    "        state = new_state\n",
    "    # Reduce epsilon\n",
    "    EPSILON = reduce_epsilon(EPSILON,episode+1)\n",
    "    # Track rewards\n",
    "    rewards.append(total_rewards)\n",
    "    # Output\n",
    "    if episode % log_interval == 0:\n",
    "        print(f\"{episode} games played; accumulated reward: {np.sum(rewards)}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "64968167-1d3f-49e9-95f5-98a81e26d382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.06288288, 3.22408724, 3.22408724, 3.06288288],\n",
       "       [3.06288288, 0.        , 3.39377604, 3.21151081],\n",
       "       [3.2115133 , 3.57239583, 3.16184041, 3.36859507],\n",
       "       [3.38873843, 0.        , 2.08398079, 2.08398079],\n",
       "       [3.22408724, 3.39377604, 0.        , 3.06288288],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 3.76041667, 0.        , 3.26783101],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [3.39377604, 0.        , 3.57239583, 3.22408724],\n",
       "       [3.39377604, 3.76041667, 3.76041667, 0.        ],\n",
       "       [3.57239583, 3.95833333, 0.        , 3.57239583],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 3.76041667, 3.95833333, 3.57239583],\n",
       "       [3.76041667, 3.95833333, 4.16666667, 3.76041667],\n",
       "       [0.        , 0.        , 3.33333333, 0.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final Q-Learning table\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8adb9e-3a22-4074-846f-91ca8e65acc8",
   "metadata": {},
   "source": [
    "**IMPORTANT persona interpretation**: Note that eventhough the reward is 1 only when we hit the G cell (i.e., close to the state 15), the Q values increase for all other states, including the ones with low values (the initial cells). That Q value propagation occurs due to the following fact: the reward and the next Q refer to the next state, but the current Q value is updated with them; thus, we can intuitively see how the Q value increases are propagated from the goal state back to the initial states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d265f4-3708-4de8-945d-1d1f6683dd96",
   "metadata": {},
   "source": [
    "## 5. Utilization and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc2a47-480e-48cf-b175-ba94b1ae24fe",
   "metadata": {},
   "source": [
    "### Utilization\n",
    "\n",
    "After training has concluded, we can use the `q_table` to take the best action given the state we are in, thus, reaching the goal the fastest possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6168f45-8f6b-42ab-80cd-095f25726af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for steps in range(100):\n",
    "    env.render()\n",
    "    action = np.argmax(q_table[state])\n",
    "    state,reward,done,info = env.step(action)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c0132-5074-4e95-9078-5640076cd8fd",
   "metadata": {},
   "source": [
    "### Training Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec3e8b3-9b46-4887-85d5-60e590d01520",
   "metadata": {},
   "source": [
    "We could visualize the training process with matplotlib. For that, we initialize the `q_table` and copy & paste the training loops for some modifications.\n",
    "\n",
    "**Note 1**: The instructor plotted an interactive diagram, but that does not work out-of-the-box in Jupyter Lab, only in Jupyter Notebooks. A solution is to follow what is said in this link:\n",
    "\n",
    "[https://stackoverflow.com/questions/51922480/javascript-error-ipython-is-not-defined-in-jupyterlab](https://stackoverflow.com/questions/51922480/javascript-error-ipython-is-not-defined-in-jupyterlab)\n",
    "\n",
    "Or, alternatively, we ca just use the following magic command and just plot the final diagram:\n",
    "\n",
    "```\n",
    "%matplotlib inline\n",
    "```\n",
    "\n",
    "**Note 2**: See how the rewards become linear with the time (=epochs). That is what we wanted to achieve: the agent basically wins every game at the end and the rewards add up in proportio to the games played!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b958422a-bba8-4706-bb34-16088adea8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([state_size,action_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e478d-ad22-47e2-9133-5491280f2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Same training loop as before, but with some plotting procedures\n",
    "rewards = []\n",
    "log_interval = 1000\n",
    "####################################\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion() # interactive mode on\n",
    "fig.canvas.draw()\n",
    "epoch_plot_tracker = []\n",
    "total_reward_plot_tracker = []\n",
    "####################################\n",
    "for episode in range(EPOCHS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(EPSILON,q_table,state)\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        old_q_value = q_table[state,action]\n",
    "        next_optimal_q_value = np.max(q_table[new_state])\n",
    "        next_q = compute_next_q_value(old_q_value,reward,next_optimal_q_value)\n",
    "        q_table[state,action] = next_q\n",
    "        total_rewards = total_rewards + reward\n",
    "        state = new_state\n",
    "    EPSILON = reduce_epsilon(EPSILON,episode+1)\n",
    "    rewards.append(total_rewards)\n",
    "    ####################################\n",
    "    total_reward_plot_tracker.append(np.sum(rewards))\n",
    "    epoch_plot_tracker.append(episode)\n",
    "    ####################################\n",
    "    if episode % log_interval == 0:\n",
    "        #print(f\"{episode} games played; accumulated reward: {np.sum(rewards)}\")\n",
    "        ax.clear()\n",
    "        ax.plot(epoch_plot_tracker,total_reward_plot_tracker)\n",
    "        fig.canvas.draw()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00741c85-9462-487a-bd62-6cc1cca815e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
