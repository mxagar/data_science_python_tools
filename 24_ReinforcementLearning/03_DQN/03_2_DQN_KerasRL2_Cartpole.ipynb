{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14556c90-c8dc-473b-a247-fce0da7639a6",
   "metadata": {},
   "source": [
    "# Keras-RL2 DQN - CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24104e-fac9-4393-81dd-046ced0f42ae",
   "metadata": {},
   "source": [
    "Usually, DQN agents are not programmed manually, as in `03_1_DQN_Manual_Cartpole.ipynb`. Instead, abstraction libraries are used on top of OpenAI Gym and Keras. There are many libraries available, the one used in the course is [Keras RL2](https://github.com/taylormcnally/keras-rl2), which requires TF >= 2.1.\n",
    "\n",
    "Note that Keras RL2 is basically Keras RL for Tensorflow 2 and that it is archived, i.e., not further developed; however, it seems to be a nice trade-off between abstraction and manual definition, optimizing for learning and understanding. Keras RL2 separates nicely:\n",
    "- the model: we just define one and internally is managed the other\n",
    "- replay memory/buffer: deque or circular array\n",
    "- policy: e.g., epsilon-greedy with decaying value\n",
    "- DQN agent: it takes all of the above and the environment and it is trained\n",
    "\n",
    "Documentation link: [https://keras-rl.readthedocs.io/en/latest/](https://keras-rl.readthedocs.io/en/latest)\n",
    "\n",
    "The documentation is not very extensive, but the examples are very nice; the examples are available on Github: [https://github.com/taylormcnally/keras-rl2/tree/master/examples](https://github.com/taylormcnally/keras-rl2/tree/master/examples)\n",
    "\n",
    "Some alternatives would be:\n",
    "- OpenAI Baselines\n",
    "- TensorFlow Agents\n",
    "\n",
    "Overview of contents:\n",
    "1. Imports and Setup\n",
    "2. Creating the ANN\n",
    "3. DQN Agent: Training\n",
    "4. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1da27-f884-42c7-a5dd-cb05b171f3b9",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3c8d2c1-7e88-46f2-82ae-4ed48c1ec6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "import gym\n",
    "from pyglet.window import key  # for manual playing\n",
    "\n",
    "# Import TF stuff first, because Keras-RL2 is built on TF\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Now the import the Keras-rl2 agent\n",
    "# See\n",
    "# https://keras-rl.readthedocs.io/en/latest/agents/overview/#available-agents\n",
    "# It is called rl, but it belongs to Keras-RL2\n",
    "from rl.agents.dqn import DQNAgent  # Use the basic Deep-Q-Network agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b968c86-f41a-4069-9169-cfe41583c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5251a88-db4a-4af3-aec0-035846cc31d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3ac5c1-b70a-4b0a-b4cf-852fe4b1d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual play\n",
    "env.reset()\n",
    "for _ in range(300):\n",
    "    env.render(mode=\"human\") # render on screen\n",
    "    random_action = env.action_space.sample() # random action\n",
    "    env.step(random_action)\n",
    "env.close() # close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649ad23-53c3-40d1-92a9-8c36bdeb3265",
   "metadata": {},
   "source": [
    "## 2. Creating the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "259def58-e45b-464a-888a-95161b9cacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6015c936-6c12-41a6-a864-587fe82ad7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72996aa4-0cec-4035-9233-d587e822c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of observations\n",
    "n_observations = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "651ce54c-489d-45b2-8c95-196d8af77389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note it is a tuple of dim 1\n",
    "# We need Flatten to address that:\n",
    "# Flatten() takes (None, a, b, c), where None is the batch,\n",
    "# and it converts it to (None, a*b*c)\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f24636dc-ac01-49b1-8b29-8c72860a8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the same model as in the previous notebook\n",
    "# but now, we also use Flatten\n",
    "model = Sequential()\n",
    "# Flatten() takes (None, a, b, c), where None is the batch,\n",
    "# and it converts it to (None, a*b*c)\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + n_observations))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_actions))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "badaca5f-40ab-4738-9132-9bfd6361139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b877279-5086-4be1-b2bd-b15d7cac750c",
   "metadata": {},
   "source": [
    "## 3. DQN Agent: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "befee724-7f5e-4ee3-8ee6-395178d239be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer = Sequential Memory\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a2a3294-16dd-4069-91b5-1736e47adc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit: the size of the deque\n",
    "# window_length: it starts making sense with images; use 1 for non-visual data\n",
    "memory = SequentialMemory(limit=20000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d888707c-e3aa-4a74-b3fd-bc801022cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "# LinearAnnealedPolicy: linear decay\n",
    "# EpsGreedyQPolicy: with a linearly decaying epsilon, choose exploitation/exploration according to it\n",
    "from rl.policy import LinearAnnealedPolicy,EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "747a567a-6887-4a5f-a2ce-6433e29a096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy of action choice\n",
    "# We use the epsilon-greedy policy, as always\n",
    "# Random (exploration) or best (exploitation) action chosen\n",
    "# depending on epsilon in [value_min, value_max], decreased by steps.\n",
    "# value_test: evaluation can be performed at a fixed epsilon (should be small: exploitation)\n",
    "# nb_steps: we match our sequential memory size\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                             attr='eps',\n",
    "                             value_max=1.0,\n",
    "                             value_min=0.1,\n",
    "                             value_test=0.05,\n",
    "                             nb_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4edd9ee-d6d3-4ddf-8105-de61c7bb8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "# We now pass all elements we have to the agent;\n",
    "# beforehand, we coded all that manually, not anymore.\n",
    "# nb_steps_warmup: our burn_in = how many steps before epsilon starts decreasing\n",
    "# target_model_update: every how many epochs do we update the weights of the frozen model\n",
    "dqn = DQNAgent(model=model,\n",
    "              nb_actions=n_actions,\n",
    "              memory=memory,\n",
    "              nb_steps_warmup=10,\n",
    "              target_model_update=100,\n",
    "              policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24d84b0b-3bad-4c2c-b659-162ed624e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Agent\n",
    "# We need to pass the optimizer for the model and the metric(s)\n",
    "# 'mae': Mean Absolute Error\n",
    "dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6a4b8f4-3a97-4b58-addd-d00c1a501576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 1.0000\n",
      "238 episodes - episode_reward: 41.815 [8.000, 200.000] - loss: 34.565 - mae: 78.555 - mean_q: 160.273 - mean_eps: 0.775\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: 1.0000\n",
      "done, took 99.197 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27eec61f348>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "# Note that it takes much much less than in the manual case, because it's optimized!\n",
    "# nb_steps: episodes\n",
    "dqn.fit(env,nb_steps=20000,visualize=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db53dfc6-6a10-45e5-9a35-3460129ffef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights in crompressed format: HDF5\n",
    "dqn.save_weights(f'dqn_{env_name}_krl2_weights.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9164ed-ebb1-40b7-bf1c-fe4a9410489e",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fe44820-9b26-497b-99e9-48c58357c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 182.000, steps: 182\n",
      "Episode 2: reward: 169.000, steps: 169\n",
      "Episode 3: reward: 132.000, steps: 132\n",
      "Episode 4: reward: 134.000, steps: 134\n",
      "Episode 5: reward: 119.000, steps: 119\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env,nb_episodes=5,visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324407ca-a29f-45e4-8170-707c511a0417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
