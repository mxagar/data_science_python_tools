{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fbce26-7021-46f6-83e0-bb582e4a3750",
   "metadata": {},
   "source": [
    "# Keras-RL2 DQN - Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a38db0-7f27-46a2-8d31-9b373364c0a7",
   "metadata": {},
   "source": [
    "This notebook implements a RL agent able to act on the [Acrobot](https://gym.openai.com/envs/Acrobot-v1/) environment: \"The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and **the goal is to swing the end of the lower link up to a given height.**\"\n",
    "\n",
    "The Github repository: [Acrobot @ Github](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py)\n",
    "\n",
    "From Github, we know:\n",
    "- State: `[cos(theta1) sin(theta1) cos(theta2) sin(theta2) thetaDot1 thetaDot2]`\n",
    "- The action is either applying +1, 0 or -1 torque on the joint between the two pendulum links.\n",
    "\n",
    "The implementation is basically a copy of the contents in the notebook\n",
    "\n",
    "`03_2_DQN_KerasRL2_Cartpole.ipynb`\n",
    "\n",
    "Overview of sections:\n",
    "1. Imports and Setup\n",
    "2. Creating the ANN\n",
    "3. DQN Agent: Training\n",
    "4. Test & Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a26f6d-05de-462e-b1d3-f21ee2995c16",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa02a656-7649-4fbd-b38e-2e3c936cab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikel\\.conda\\envs\\ds\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "import numpy as np\n",
    "import gym\n",
    "from pyglet.window import key  # for manual playing\n",
    "\n",
    "# Import TF stuff first, because Keras-RL2 is built on TF\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Now the import the Keras-rl2 agent\n",
    "# See\n",
    "# https://keras-rl.readthedocs.io/en/latest/agents/overview/#available-agents\n",
    "# It is called rl, but it belongs to Keras-RL2\n",
    "from rl.agents.dqn import DQNAgent  # Use the basic Deep-Q-Network agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a91a1fa-a0db-4db0-b8d8-58bc093cbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed210c6e-b0e5-49fa-8ec0-45fc50117a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc017e8-1dc2-478b-963a-d917d3d8383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual play\n",
    "env.reset()\n",
    "for _ in range(300):\n",
    "    env.render(mode=\"human\") # render on screen\n",
    "    random_action = env.action_space.sample() # random action\n",
    "    env.step(random_action)\n",
    "env.close() # close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1023fee-f535-4781-8699-877ed641f2be",
   "metadata": {},
   "source": [
    "## 2. Creating the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8a71383-c5e4-4234-a29d-579a99a457ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1378b218-817a-4e25-bc89-9962b6d71cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83786d81-c9f8-4b44-b1f0-1eb95fc93a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of observations\n",
    "n_observations = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9551552d-3ce5-4344-84d2-b3ed183e269b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note it is a tuple of dim 1\n",
    "# We need Flatten to address that:\n",
    "# Flatten() takes (None, a, b, c), where None is the batch,\n",
    "# and it converts it to (None, a*b*c)\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12176d5b-54ec-41d2-8066-48e69ec6f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar model as before, but with 64 units in each of the three layers\n",
    "model = Sequential()\n",
    "# Flatten() takes (None, a, b, c), where None is the batch,\n",
    "# and it converts it to (None, a*b*c)\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + n_observations))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_actions))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd461898-7320-42d2-9a2d-729c2fedfbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                448       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46a9c9-5443-4f6c-8a9e-dd6fabbf2906",
   "metadata": {},
   "source": [
    "## 3. DQN Agent: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d855247-11fe-4aba-a17f-91a57ca1ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer = Sequential Memory\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f18b85d-5d2f-402b-bb7b-d8fb600a4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit: the size of the deque\n",
    "# window_length: it starts making sense with images; use 1 for non-visual data\n",
    "memory = SequentialMemory(limit=50000, window_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f45917c9-eae9-4da3-86e4-1b42401a910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "# LinearAnnealedPolicy: linear decay\n",
    "# EpsGreedyQPolicy: with a linearly decaying epsilon, choose exploitation/exploration according to it\n",
    "from rl.policy import LinearAnnealedPolicy,EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c417be1a-eb85-4cfa-8834-53bacd57dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy of action choice\n",
    "# We use the epsilon-greedy policy, as always\n",
    "# Random (exploration) or best (exploitation) action chosen\n",
    "# depending on epsilon in [value_min, value_max], decreased by steps.\n",
    "# value_test: evaluation can be performed at a fixed epsilon (should be small: exploitation)\n",
    "# nb_steps: we match our sequential memory size\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                             attr='eps',\n",
    "                             value_max=1.0,\n",
    "                             value_min=0.1,\n",
    "                             value_test=0.05,\n",
    "                             nb_steps=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c1b6d8-9060-4ba6-8e1d-e0b5dd06faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "# We now pass all elements we have to the agent;\n",
    "# beforehand, we coded all that manually, not anymore.\n",
    "# nb_steps_warmup: our burn_in = how many steps before epsilon starts decreasing\n",
    "# target_model_update: every how many epochs do we update the weights of the frozen model\n",
    "# Optional: batch_size, gamma\n",
    "dqn = DQNAgent(model=model,\n",
    "              nb_actions=n_actions,\n",
    "              memory=memory,\n",
    "              nb_steps_warmup=1000,\n",
    "              target_model_update=1000,\n",
    "              batch_size=32,\n",
    "              gamma=0.99, \n",
    "              policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41bc6a67-93d3-4302-878d-49fab75145b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Agent\n",
    "# We need to pass the optimizer for the model and the metric(s)\n",
    "# 'mae': Mean Absolute Error\n",
    "dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e603529-485d-45fc-b8f9-00cab16ea15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  265/10000 [..............................] - ETA: 5s - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mikel\\.conda\\envs\\ds\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 46s 5ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.483 - mae: 0.327 - mean_q: 0.000 - mean_eps: 0.967\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -493.400 [-500.000, -368.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.910\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -495.050 [-500.000, -401.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.850\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -494.850 [-500.000, -397.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.790\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.730\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.670\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -498.850 [-500.000, -477.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.610\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -496.000 [-500.000, -420.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.550\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -0.9997\n",
      "21 episodes - episode_reward: -484.286 [-500.000, -331.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.490\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.9999\n",
      "20 episodes - episode_reward: -493.250 [-500.000, -365.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.430\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.370\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.9997\n",
      "21 episodes - episode_reward: -491.048 [-500.000, -410.000] - loss: 0.500 - mae: 0.333 - mean_q: 0.000 - mean_eps: 0.310\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      " 6578/10000 [==================>...........] - ETA: 22s - reward: -1.0000"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# Note that it takes much much less than in the manual case, because it's optimized!\n",
    "# nb_steps: episodes\n",
    "dqn.fit(env,nb_steps=150000,visualize=False,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5722f6-10f2-40a4-b631-a65755b556b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights in crompressed format: HDF5\n",
    "dqn.save_weights(f'dqn_{env_name}_krl2_weights.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e92042-0aff-49f0-b136-74ab3eaa665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "# Note that we need to create the model and the DQN agent before loading the weights!\n",
    "dqn.load_weights(f'dqn_{env_name}_krl2_weights.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c784b3-97f4-4399-a770-956c0a328fdd",
   "metadata": {},
   "source": [
    "## 4. Test & Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb569be3-cbfb-4bd7-bd1e-b4a077a6b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "dqn.test(env,nb_episodes=5,visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58dc92d-9fbf-493b-8a9a-a6c8c5621ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to carry out actions without Keras-RL2, only with the model\n",
    "observation = env.reset()\n",
    "for counter in range(2000):\n",
    "    env.render()\n",
    "    print()\n",
    "    action = np.argmax(model.predict(observation.reshape((1,1,6))))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        #pass\n",
    "        #print('done')\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
