{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa63394d-08a5-4802-baf8-6b41ac34bc37",
   "metadata": {},
   "source": [
    "# Manual DQN - Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5fca9-07e9-48a0-ba2b-55cee044ca3a",
   "metadata": {},
   "source": [
    "This notebook implements the **Deep Q Network (DQN)** for the [CartPole](https://gym.openai.com/envs/CartPole-v1/) game manually.\n",
    "See `../ReinforcementLearning_Guide.md` for theory and intuition.\n",
    "\n",
    "DQN build on the concepts introduced in Q Learning; see `../02_QLearning` for examples.\n",
    "\n",
    "According to the OpenAI environment page of CartPole: \"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force to the left (0) or right (1) of the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright (i.e., does not fall). The episode ends when the pole is more than 12 degrees from vertical, or the cart moves more than 2.4 units from the center.\"\n",
    "\n",
    "Note that I needed to change the description above, since it is outdated on the web page.\n",
    "\n",
    "Look at the Github page of the environment, on the docstring of the environment.\n",
    "\n",
    "Note the following:\n",
    "- The center position is 0 and the range of possibles postions is `[-2.4,2.4]`, continuous\n",
    "- Pole angle can vary in `[-12,12] deg`\n",
    "- Velocity (linear for cart, angular for pole) can be any\n",
    "- An episode is done if\n",
    "    1. The pole tresspasses the limits above\n",
    "    2. 200 steps/actions taken\n",
    "    3. A minimum return is achieved over 100 steps/actions\n",
    "\n",
    "We need to discretize the domains using bins??\n",
    "\n",
    "Overview of sections:\n",
    "1. Environment Setup\n",
    "2. Neural Networks: Q Network & Target Network\n",
    "3. Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3df6ab-9c97-4e68-b2b1-cfa8f45e99f0",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72aff0ce-a177-4b53-b3e5-5600af3c2d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxagar/opt/anaconda3/envs/tf/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb192086-234d-42b0-8aa0-8c1d1327f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,clone_model\n",
    "from tensorflow.keras.layers import Dense,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "536a6998-f5e2-455f-b9ed-b42a8087770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8aef455-aa20-47d4-89b7-be877f07d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 10:11:08.027 python[26966:8363008] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fd0e5bc86f0>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-23 10:11:08.027 python[26966:8363008] Warning: Expected min height of view: (<NSButton: 0x7fd0e5976630>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-23 10:11:08.029 python[26966:8363008] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fd0e5875b10>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "2022-01-23 10:11:08.030 python[26966:8363008] Warning: Expected min height of view: (<NSPopoverTouchBarItemButton: 0x7fd0e5876890>) to be less than or equal to 30 but got a height of 32.000000. This error will be logged once per view in violation.\n",
      "/Users/mxagar/opt/anaconda3/envs/tf/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py:151: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  \"You are calling 'step()' even though this \"\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for step in range(1000):\n",
    "    env.render(mode='human')\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e63b9-023c-44f6-9c7a-482ec6d849f5",
   "metadata": {},
   "source": [
    "## 2. Neural Networks: Q Network & Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f30d501c-79af-4f8e-963f-b1781c21a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of observations (alternatively: Github)\n",
    "num_observations = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "549af1e6-984b-47fa-8aec-201ada6e7fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f4c5519-a124-4dec-8a97-de6da0a50abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of actions (alternatively: Github)\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9cbb69a-45d1-41ad-91f8-21da7472d2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30cd06c7-36dc-4ded-ba0b-bfc554406b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, our ANN is defined with these input/output sizes\n",
    "# input_shape = num_observations = 4 (state) --> (leyers) --> output neurons = num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "be307e39-5203-4506-8211-f705756db0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Network\n",
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=[1,4]))\n",
    "# A Dense layer of 16 (4x) units/neurons which receives as input num_observations values (4)\n",
    "model.add(Dense(16,input_shape=(1,num_observations)))\n",
    "model.add(Activation('relu')) # we can add it in the Dense layer\n",
    "# Expand: 2x\n",
    "model.add(Dense(32,activation='relu'))\n",
    "# Final output layer\n",
    "model.add(Dense(num_actions, activation='linear')) # no change, ie. f(x) = x ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "50f530ea-a984-4904-a839-4302f7ad1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1, 16)             80        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1, 32)             544       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1, 2)              66        \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "68bf5a8d-6348-4bd4-87ed-83e0634f348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Network\n",
    "target_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a4d6bc-8562-4698-be46-b6684f8d406b",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters and Functions (Epsilon-Greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e86e757a-89c9-4d17-b17a-16182c0703f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "EPSILON = 1.0\n",
    "# Another way of reducing epsilon (exploration)\n",
    "# is to multiply to it a value close but under 1 every step\n",
    "espsilon_reduce = 0.995\n",
    "LEARNING_RATE = 0.001 # Watch out: not the ALPHA from Q Learning, but the LR of the NN!\n",
    "GAMMA = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e3df4689-0b52-47c4-8bf7-5cd4c49b9982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation):\n",
    "    if np.random.random() > epsilon: # Exploit\n",
    "        # Predict\n",
    "        prediction = model.predict(observation)\n",
    "        # Select action with highest Q value\n",
    "        action = np.argmax(prediction)\n",
    "    else: # Explore\n",
    "        action = np.random.randint(0, env.action_space.n)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35d32a-f761-4b28-8a71-18996bdbfe69",
   "metadata": {},
   "source": [
    "## 3. Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ff5b8-bc81-493a-a805-1584a0d315d6",
   "metadata": {},
   "source": [
    "### Deques\n",
    "\n",
    "Let's analyze deques first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "71650b5e-b3b5-41af-9b1d-08cafba9ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a deque of size 5\n",
    "deque_1 = deque(maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0c38056e-3ddc-45f1-bea0-b867177b4ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The deque is empty\n",
    "deque_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76c1d1f5-c934-4e56-a2b6-1c4d124a4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add/append 5 elements (maximum) to the deque: [0, 1, 2, 3, 4]\n",
    "for i in range(5):\n",
    "    deque_1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d5efc05b-3623-4a0c-9d43-6a40ec350da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deque_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "46dbbc66-4e00-4667-b130-831ff0cf5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add/append another 6th element\n",
    "# First/Oldest is removed from head, Last/Newest is added to tail\n",
    "deque_1.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "28b384a2-bc6d-4d52-b99c-6417cad7966a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deque_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a92b5-06a7-4c88-8148-4451b56e9fef",
   "metadata": {},
   "source": [
    "### Tuples Management\n",
    "\n",
    "In the following, a one-liner for re-combining elements from tuples is shown, used later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "976da466-aa85-40dd-857a-0a7729036293",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuple = [(1,2,3),(4,5,6),(7,8,9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "68f77110-115d-49ed-a121-c0b09ee4362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * iterates through the list elements\n",
    "# zip(): the elements of passed collections are merged\n",
    "# into tuples that contain elements from the different lists\n",
    "# list(): zip is a generator, we need to convert it to a list\n",
    "zipped_list = list(zip(*test_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cb735ace-d2de-499e-aa2b-e83c22935eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 7) (2, 5, 8) (3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "# unpack\n",
    "a, b, c = zipped_list\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6064208-3e99-4767-9aa5-f70cf3ef07df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45eb2b-078b-4023-b04a-7aa5f9767579",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** As it is written in the `replay()` function, I think there is a misunderstanding either here in the code or on the notes/guide. See the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5f1de110-f789-4ad0-8541-c266e418c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often do we update the target model?\n",
    "# Number of epochs\n",
    "# Take into account that each replay trains for one epoch only\n",
    "update_target_model = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ac7e2a69-8c27-4f77-8a5e-88f438084184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer itself\n",
    "replay_buffer = deque(maxlen=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d75e762d-9fd0-4f51-94fd-c77de91a1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    # If the buffer is at least the size of the training subset or batch, \n",
    "    # no training done\n",
    "    if len(replay_buffer) < batch_size: \n",
    "        return\n",
    "    # If buffer already filled until batch size, training done!\n",
    "    # Get experience samples: exp = (state, action, reward, new_state, done)\n",
    "    samples = random.sample(replay_buffer, batch_size)  \n",
    "    # Recombine and put each class-element of experience tuples together\n",
    "    zipped_samples = list(zip(*samples))\n",
    "    # unpack\n",
    "    states, actions, rewards, new_states, dones = zipped_samples\n",
    "    # WARNING: Either in my notes or here, something is wrong?\n",
    "    # The target model is fed with S_t or S_t+1?\n",
    "    # The Q network is fed with S_t+1 or S_t?\n",
    "    # The output targets of target_model is not being used...\n",
    "    # ... its elements are copied but then replaced!?\n",
    "    targets = target_model.predict(np.array(states))\n",
    "    q_values = model.predict(np.array(new_states))\n",
    "    target_batch = []\n",
    "    for i in range(batch_size):  \n",
    "        q_value = max(q_values[i][0])\n",
    "        #q_value = max(q_values[i])\n",
    "        target = targets[i].copy()  \n",
    "        if dones[i]:\n",
    "            target[0][actions[i]] = rewards[i]\n",
    "            #target[actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]] = rewards[i] + q_value * GAMMA\n",
    "            #target[actions[i]] = rewards[i] + q_value * GAMMA\n",
    "        target_batch.append(target)\n",
    "    # Train for 1 epoch only\n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "631cc70f-a2a6-4d2f-baa5-24006408f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model update\n",
    "def update_model_handler(epoch, update_target_model, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model == 0:\n",
    "        # Get weights from Q network and copy them to the target network\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ca322-9a65-4427-802a-e2162038a4a5",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c77bb6-362c-4f25-aaa1-48a969da8b60",
   "metadata": {},
   "source": [
    "The training part is very similar to the one in Q learning.\n",
    "\n",
    "**IMPORTANT NOTE**: \n",
    "\n",
    "I have not achieved to train with the following one on my Apple M1.\n",
    "The error is related to the input_shape, which is not the expected one.\n",
    "\n",
    "    ValueError: Error when checking input: expected dense_input to have 3 dimensions, but got array with shape (1, 4)\n",
    "\n",
    "It seems that using TF 2.6.7 could solve it, but I cannot use that version on a M1 chip -- or at least, it is not as straighforward as it should be.\n",
    "For more information, see:\n",
    "\n",
    "[https://www.udemy.com/course/practical-ai-with-python-and-reinforcement-learning/learn/lecture/27376754#questions/16362492](https://www.udemy.com/course/practical-ai-with-python-and-reinforcement-learning/learn/lecture/27376754#questions/16362492)\n",
    "\n",
    "See also:\n",
    "\n",
    "[https://stackoverflow.com/questions/67000544/valueerror-error-when-checking-input-expected-dense-input-to-have-2-dimensions](https://stackoverflow.com/questions/67000544/valueerror-error-when-checking-input-expected-dense-input-to-have-2-dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce97b4b6-fb75-4462-ad75-c14572ed0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model, since it was not done yet...\n",
    "# Compile = configure model for training\n",
    "# Note that the \n",
    "# Watch out: not the ALPHA from Q Learning, but the LR of the NN!\n",
    "model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f6d6a-99cb-4d94-a9cb-7f9fd301f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the points: epoch with the max number of points recorded\n",
    "best_so_far = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    observation = env.reset()\n",
    "    # Our observations are [a,b,c,d] size (4,)\n",
    "    # Keras expects shape (1,4)\n",
    "    observation = observation.reshape([1, 4])\n",
    "    done = False  \n",
    "    points = 0\n",
    "    while not done:\n",
    "        # Compute action\n",
    "        action = epsilon_greedy_action_selection(model, EPSILON, observation)\n",
    "        # Execute action\n",
    "        next_observation, reward, done, info = env.step(action)  \n",
    "        # Next state/observation\n",
    "        next_observation = next_observation.reshape([1, 4])\n",
    "        # Replay buffer: add experience!\n",
    "        replay_buffer.append((observation, action, reward, next_observation, done))\n",
    "        # Update loop variables\n",
    "        observation = next_observation\n",
    "        points+=1\n",
    "        # Train the model!\n",
    "        # batch_size = 32\n",
    "        replay(replay_buffer, 32, model, target_model)\n",
    "    # After one epoch, reduce EPSILON: exploration -> exploitation\n",
    "    EPSILON *= espsilon_reduce\n",
    "    # Refresh Q network weights every N=update_target_model epochs\n",
    "    update_model_handler(epoch, update_target_model, model, target_model)\n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"{epoch}: Points reached: {points} - epsilon: {EPSILON} - Best: {best_so_far}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab8f8b-5835-48eb-996e-f1ed034cecf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214533c-5cd0-438e-8317-5b82f8913c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
